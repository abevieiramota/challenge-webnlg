### WebNLG Human Evaluation
The repository contains files used for human evaluation in the WebNLG Challenge.

More details can be found in the report on the results page:
http://webnlg.loria.fr/pages/results.html


We sampled 223 (data, text) pairs for human evaluation.

* _sample-ids.txt_ contains sampled ids from test set
* _MRs.txt_ contains data
* _gold-sample-reference*.lex_ contains references
* _all\_data\_final\_averaged.csv_ contains human evaluation results. Fluency, Semantics, and Grammar columns represent average scores that we got by collecting three human judgments. References are denoted by "webnlg"; their automatic scores are always 1.0 or 0.0.

