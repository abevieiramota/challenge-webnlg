{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "\n",
    "import webnlg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train', 'test_no_lex', 'test_unseen_with_lex', 'test_with_lex', 'dev_1.2', 'train_1.2'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webnlg.DATASETS_FILEPATHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = webnlg.load(dataset=['dev', 'train', 'test_with_lex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: category=Food eid=Id213\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Philippines | ethnicGroup | Ilocano_people\n",
       "Philippines | language | Arabic\n",
       "Philippines | ethnicGroup | Zamboangans\n",
       "Philippines | language | Philippine_Spanish\n",
       "Batchoy | country | Philippines\n",
       "\n",
       "\n",
       "\tLexicalizations:\n",
       "\n",
       "Batchoy is eaten in the Philippines where Philippine Spanish and Arabic are spoken. It is also where the Zamboangans and Ilocano people are two of the ethnic groups.\n",
       "\n",
       "\n",
       "Zamboangans and Ilocano people are ethnic groups from the Philippines which is home to the dish batchoy and where Philippine Spanish and Arabic are spoken.\n",
       "\n",
       "\n",
       "Batchoy comes from the Philippines, where Philippine Spanish and Arabic are among the languages spoken, and the Zamboangans and IIocano people are ethnic groups.\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = corpus.sample(seed=300)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing(iterable):\n",
    "    \n",
    "    for i, item1 in enumerate(iterable):\n",
    "        \n",
    "        for j, item2 in enumerate(iterable):\n",
    "            \n",
    "            if i != j:\n",
    "                \n",
    "                yield (item1, item2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Batchoy is eaten in the Philippines where Philippine Spanish and Arabic are spoken. It is also where the Zamboangans and Ilocano people are two of the ethnic groups.',\n",
       "  'Zamboangans and Ilocano people are ethnic groups from the Philippines which is home to the dish batchoy and where Philippine Spanish and Arabic are spoken.'),\n",
       " ('Batchoy is eaten in the Philippines where Philippine Spanish and Arabic are spoken. It is also where the Zamboangans and Ilocano people are two of the ethnic groups.',\n",
       "  'Batchoy comes from the Philippines, where Philippine Spanish and Arabic are among the languages spoken, and the Zamboangans and IIocano people are ethnic groups.'),\n",
       " ('Zamboangans and Ilocano people are ethnic groups from the Philippines which is home to the dish batchoy and where Philippine Spanish and Arabic are spoken.',\n",
       "  'Batchoy is eaten in the Philippines where Philippine Spanish and Arabic are spoken. It is also where the Zamboangans and Ilocano people are two of the ethnic groups.'),\n",
       " ('Zamboangans and Ilocano people are ethnic groups from the Philippines which is home to the dish batchoy and where Philippine Spanish and Arabic are spoken.',\n",
       "  'Batchoy comes from the Philippines, where Philippine Spanish and Arabic are among the languages spoken, and the Zamboangans and IIocano people are ethnic groups.'),\n",
       " ('Batchoy comes from the Philippines, where Philippine Spanish and Arabic are among the languages spoken, and the Zamboangans and IIocano people are ethnic groups.',\n",
       "  'Batchoy is eaten in the Philippines where Philippine Spanish and Arabic are spoken. It is also where the Zamboangans and Ilocano people are two of the ethnic groups.'),\n",
       " ('Batchoy comes from the Philippines, where Philippine Spanish and Arabic are among the languages spoken, and the Zamboangans and IIocano people are ethnic groups.',\n",
       "  'Zamboangans and Ilocano people are ethnic groups from the Philippines which is home to the dish batchoy and where Philippine Spanish and Arabic are spoken.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pairing(sample.lexes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paraphrases.txt', 'w') as f:\n",
    "    \n",
    "    f.write('input\\toutput\\n')\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for entry in corpus:\n",
    "        \n",
    "        for pair in pairing(entry.lexes()):\n",
    "            \n",
    "            f.write('{}\\t{}\\n'.format(*pair))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delexicalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "delex = webnlg.load(dataset=['dev_1.2', 'train_1.2'])\n",
    "\n",
    "with open('paraphrases_delex.txt', 'w') as f:\n",
    "    \n",
    "    f.write('input\\toutput\\n')\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for entry in delex:\n",
    "        \n",
    "        for pair in pairing([l['template'] for l in entry.entry['lexes']]):\n",
    "            \n",
    "            f.write('{}\\t{}\\n'.format(*pair))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\toutput\r\n",
      "AGENT-1 was created by PATIENT-1 .\tThe creator of AGENT-1 is PATIENT-1 .\r\n",
      "AGENT-1 was created by PATIENT-1 .\tAGENT-1 , was created by PATIENT-1 .\r\n",
      "The creator of AGENT-1 is PATIENT-1 .\tAGENT-1 was created by PATIENT-1 .\r\n",
      "The creator of AGENT-1 is PATIENT-1 .\tAGENT-1 , was created by PATIENT-1 .\r\n",
      "AGENT-1 , was created by PATIENT-1 .\tAGENT-1 was created by PATIENT-1 .\r\n",
      "AGENT-1 , was created by PATIENT-1 .\tThe creator of AGENT-1 is PATIENT-1 .\r\n",
      "AGENT-1 was created by PATIENT-1 .\tAGENT-1 was created by PATIENT-1 .\r\n",
      "AGENT-1 was created by PATIENT-1 .\tAGENT-1 was created by PATIENT-1 .\r\n",
      "AGENT-1 was created by PATIENT-1 .\tAGENT-1 was created by PATIENT-1 .\r\n"
     ]
    }
   ],
   "source": [
    "!head paraphrases_delex.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting OpenNMT-tf[tensorflow_gpu]\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/b1/bc455ed9d4c4eaa5ca7151903010607c1d22233634c298883f1ec327d224/OpenNMT_tf-1.22.0-py2.py3-none-any.whl (148kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 1.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml (from OpenNMT-tf[tensorflow_gpu])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting rouge==0.3.1 (from OpenNMT-tf[tensorflow_gpu])\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /simple/rouge/\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/89/af359c22e1d858e0299d4cc9219f36b504817c9797acad23081247867845/rouge-0.3.1-py3-none-any.whl\n",
      "Collecting sacrebleu==1.*; python_version >= \"3.0\" (from OpenNMT-tf[tensorflow_gpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/12/5b/7196b11bca204cb6ca9000b5dc910e809081f224c73ef28e9991080e4e51/sacrebleu-1.3.1.tar.gz\n",
      "Collecting pyonmttok<2,>=1.11.0; platform_system == \"Linux\" (from OpenNMT-tf[tensorflow_gpu])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/02/27e56aea1ce80d8b82f7372ca41fa5f25dc6ca89962768d1a1ca8d329e57/pyonmttok-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\" (from OpenNMT-tf[tensorflow_gpu])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 345.2MB 44kB/s  eta 0:00:01    36% |███████████▉                    | 127.4MB 12.9MB/s eta 0:00:17.6MB 52.0MB/s eta 0:00:03    68% |█████████████████████▉          | 235.8MB 11.4MB/s eta 0:00:10    69% |██████████████████████▏         | 238.8MB 10.0MB/s eta 0:00:11\n",
      "\u001b[?25hCollecting typing (from sacrebleu==1.*; python_version >= \"3.0\"->OpenNMT-tf[tensorflow_gpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.12.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.13.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.15.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.32.3)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.2.2)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (3.6.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.19.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (2.0.0)\n",
      "Requirement already satisfied: h5py in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (40.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (5.1.3)\n",
      "Building wheels for collected packages: pyyaml, sacrebleu\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/abevieiramota/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
      "  Running setup.py bdist_wheel for sacrebleu ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/abevieiramota/.cache/pip/wheels/56/c0/fb/1c7f9b3a71f64cdf86291cc645596f71746807bf2f72b3c1dd\n",
      "Successfully built pyyaml sacrebleu\n",
      "Installing collected packages: pyyaml, rouge, typing, sacrebleu, pyonmttok, tensorflow-gpu, OpenNMT-tf\n",
      "Successfully installed OpenNMT-tf-1.22.0 pyonmttok-1.11.0 pyyaml-5.1 rouge-0.3.1 sacrebleu-1.3.1 tensorflow-gpu-1.13.1 typing-3.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install OpenNMT-tf[tensorflow_gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "delex = webnlg.load(dataset=['dev_1.2', 'train_1.2'])\n",
    "\n",
    "with open('paraphrases_delex_src.txt', 'w') as f_src, open('paraphrases_delex_tgt.txt', 'w') as f_tgt:\n",
    "    \n",
    "    all_pairs = set()\n",
    "    \n",
    "    for entry in delex:\n",
    "        \n",
    "        for pair in pairing([l['template'] for l in entry.entry['lexes']]):\n",
    "            \n",
    "            all_pairs.add(pair)\n",
    "            \n",
    "    for pair in all_pairs:\n",
    "            \n",
    "        f_src.write('{}\\n'.format(pair[0]))\n",
    "        f_tgt.write('{}\\n'.format(pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT-1 play at BRIDGE-1 in the city of BRIDGE-2 , PATIENT-1 .\r\n",
      "AGENT-1 is located in PATIENT-1 in BRIDGE-1 .\r\n",
      "AGENT-1 ground is in PATIENT-1 and AGENT-1 compete in PATIENT-2 .\r\n",
      "A traditional dish of PATIENT-1 is called AGENT-1 , which has PATIENT-2 as an ingredient and should be served PATIENT-3 .\r\n",
      "AGENT-1 , established in PATIENT-3 , is located in BRIDGE-1 , PATIENT-1 and falls under the category of PATIENT-5 . PATIENT-4 is to the north of BRIDGE-1 and to the southeast of BRIDGE-1 lies PATIENT-2 .\r\n",
      "PATIENT-4 are an ethnic group in BRIDGE-1 which uses PATIENT-1 . The leader is PATIENT-3 who has the title of PATIENT-2 . BRIDGE-1 is the location of AGENT-1 .\r\n",
      "AGENT-1 , completed in PATIENT-2 , has a floor count of PATIENT-1 .\r\n",
      "AGENT-1 (PATIENT-3) , published by PATIENT-1 has the LCCN number PATIENT-2 and CODEN code PATIENT-4 .\r\n",
      "PATIENT-1 is the area of water in AGENT-1 .\r\n",
      "AGENT-1 , PATIENT-2 , has the Architectural style of PATIENT-1 .\r\n"
     ]
    }
   ],
   "source": [
    "!head paraphrases_delex_src.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT-1 ground is BRIDGE-1 which is located in the city of BRIDGE-2 , PATIENT-1\r\n",
      "BRIDGE-1 is in PATIENT-1 . AGENT-1 is located BRIDGE-1 .\r\n",
      "AGENT-1 grounds are in PATIENT-1 and obviously AGENT-1 play in PATIENT-2 .\r\n",
      "PATIENT-2 is an ingredient of the PATIENT-1 dish AGENT-1 , which should be served PATIENT-3 .\r\n",
      "AGENT-1 (location BRIDGE-1 , PATIENT-1) was completed in PATIENT-3 and falls under the category of PATIENT-5 . To the north of BRIDGE-1 lies PATIENT-4 and to the southeast lies PATIENT-2 .\r\n",
      "AGENT-1 is located in BRIDGE-1 , where PATIENT-1 is spoken , and one of the ethnic groups is PATIENT-4 . PATIENT-2 is the most important leader , and PATIENT-3 is also a leader .\r\n",
      "AGENT-1 was completed in PATIENT-2 and has PATIENT-1 floors .\r\n",
      "AGENT-1 was published by PATIENT-1 and is abbreviated PATIENT-3 . The CODEN code is `` PATIENT-4 '' and the LCCN number is PATIENT-2 .\r\n",
      "AGENT-1 has an area of water that is PATIENT-1 .\r\n",
      "AGENT-1 which is located in PATIENT-2 has the architectural style of PATIENT-1 .\r\n"
     ]
    }
   ],
   "source": [
    "!head paraphrases_delex_tgt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/imp.py\", line 243, in load_module\r\n",
      "    return load_dynamic(name, filename, file)\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n",
      "    return _load(spec)\r\n",
      "ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/bin/onmt-build-vocab\", line 7, in <module>\r\n",
      "    from opennmt.bin.build_vocab import main\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/opennmt/__init__.py\", line 5, in <module>\r\n",
      "    from opennmt import decoders\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/opennmt/decoders/__init__.py\", line 3, in <module>\r\n",
      "    from opennmt.decoders.rnn_decoder import RNNDecoder\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/opennmt/decoders/rnn_decoder.py\", line 7, in <module>\r\n",
      "    import tensorflow as tf\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n",
      "    raise ImportError(msg)\r\n",
      "ImportError: Traceback (most recent call last):\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/imp.py\", line 243, in load_module\r\n",
      "    return load_dynamic(name, filename, file)\r\n",
      "  File \"/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n",
      "    return _load(spec)\r\n",
      "ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n",
      "\r\n",
      "\r\n",
      "Failed to load the native TensorFlow runtime.\r\n",
      "\r\n",
      "See https://www.tensorflow.org/install/errors\r\n",
      "\r\n",
      "for some common reasons and solutions.  Include the entire stack trace\r\n",
      "above this error message when asking for help.\r\n"
     ]
    }
   ],
   "source": [
    "!onmt-build-vocab --size 50000 --save-vocab src-vocab.txt paraphrases_delex_src.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
