{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "from webnlg import WebNLGCorpus\n",
    "from textacy import extract, spacy_utils, preprocess\n",
    "from textacy import similarity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['ner'])\n",
    "\n",
    "# bug? https://github.com/explosion/spaCy/issues/1574#issuecomment-391732372\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    for w in (word, word[0].upper() + word[1:], word.upper()):\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "\n",
    "train = WebNLGCorpus.load('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "* LOG AND TRACK CASES WHERE THE ALGORITHM WASN'T ABLE TO DELEXICALIZE ALL PREDICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6940/6940 [09:29<00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 54s, sys: 3.7 s, total: 36min 58s\n",
      "Wall time: 9min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "def test_if_overlaps_with(tested_span, to_test_span=None):\n",
    "    \n",
    "    # doesn't overlap with None\n",
    "    if not to_test_span:\n",
    "        return False\n",
    "    \n",
    "    # tests if the spans have overlap\n",
    "    return tested_span.start_char <= to_test_span.end_char and to_test_span.start_char <= tested_span.end_char\n",
    "\n",
    "translate_punct_to_none = str.maketrans({k: ' ' for k in string.punctuation})\n",
    "\n",
    "def preprocess_text_to_compare(s):\n",
    "    \n",
    "    return s.translate(translate_punct_to_none)\n",
    "    \n",
    "\n",
    "# regex to replace sequences of dots to a single dot -> \n",
    "#    there are cases of, I think, typos of two dots(OMG I've just thought that maybe it was a case of <abbreviature, final dot>)\n",
    "c_dot = re.compile(r'\\.{1,}')\n",
    "# regex to extract PREDICATE tags\n",
    "#    they are put in the format PREDICATE-$predicate_string$\n",
    "#    and I need to find them and extract the predicate_string part\n",
    "c_predicate = re.compile(r'PREDICATE-\\$(.*?)\\$')\n",
    "\n",
    "c_remove_lowercase = re.compile(r'[^A-Z]')\n",
    "\n",
    "def extract_from_entry(entry, ngram_lim=(1, 12), threshold_full=0.5, threshold_abbrev=0.5,\n",
    "                       similarity_full=similarity.levenshtein,\n",
    "                       similarity_abbrev=similarity.levenshtein):\n",
    "    \n",
    "    positions = []\n",
    "\n",
    "    # for each lexicalization\n",
    "    for text in entry.lexes():\n",
    "        # creates an array of chars from the lexicalization\n",
    "        #    used to replace objects strings by tags\n",
    "        #    because, in Python, str is immutable\n",
    "        text_char = list(text)\n",
    "\n",
    "        # creates a doc of the lexicalization\n",
    "        doc = nlp(c_dot.sub('.', text))\n",
    "\n",
    "        ngrams = []\n",
    "        for n in range(*ngram_lim):\n",
    "            ngrams.extend(extract.ngrams(doc, n, filter_punct=False, filter_stops=False, filter_nums=False))\n",
    "\n",
    "        sims = []\n",
    "\n",
    "        for ngram in ngrams:\n",
    "            \n",
    "            ngram_preprocessed = preprocess_text_to_compare(ngram.text)\n",
    "\n",
    "            for triple in entry.get_data():\n",
    "                \n",
    "                object_preprocessed = preprocess_text_to_compare(triple['object'])\n",
    "                object_abbrev_preprocessed = c_remove_lowercase.sub('', object_preprocessed)\n",
    "\n",
    "                sims.append({'ngram': ngram,\n",
    "                             'predicate': triple['predicate'],\n",
    "                             'object': triple['object'],\n",
    "                             'sim': similarity_full(ngram_preprocessed, object_preprocessed),\n",
    "                             'sim_abbrev': similarity_abbrev(ngram_preprocessed, object_abbrev_preprocessed)})\n",
    "\n",
    "        df = pd.DataFrame(sims)\n",
    "        \n",
    "        choosen_ngram = None\n",
    "        choosen_rows = []\n",
    "        \n",
    "        predicates_objects = [(data['predicate'], len(data['object'])) for data in entry.get_data()]\n",
    "        sorted_predicates = [v[0] for v in sorted(predicates_objects, key=lambda v: v[1], reverse=True)]\n",
    "        \n",
    "        g_ = df\n",
    "        \n",
    "        for predicate in sorted_predicates:\n",
    "            \n",
    "            # removes overlaps\n",
    "            g_ = g_[~g_.ngram.apply(lambda n: test_if_overlaps_with(n, choosen_ngram))]\n",
    "            \n",
    "            choosen_row = g_[g_.predicate == predicate].nlargest(1, 'sim')\n",
    "            \n",
    "            if choosen_row.sim.values[0] < threshold_full:\n",
    "                \n",
    "                choosen_row = g_[g_.predicate == predicate].nlargest(1, 'sim_abbrev')\n",
    "                \n",
    "                if choosen_row.sim_abbrev.values[0] < threshold_abbrev:\n",
    "                    choosen_ngram = None\n",
    "                    continue\n",
    "\n",
    "            choosen_ngram = choosen_row.ngram.values[0]\n",
    "            choosen_rows.append(choosen_row.index.values[0])\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        g = df.loc[choosen_rows, :]\n",
    "        \n",
    "        g['end_char'] = g.ngram.apply(lambda x: x.end_char)\n",
    "        \n",
    "        for idx, row in g.sort_values('end_char', ascending=False).iterrows():\n",
    "\n",
    "            text_char[row['ngram'].start_char: row['ngram'].end_char] = f'PREDICATE-${row[\"predicate\"]}$'\n",
    "\n",
    "        final = ''.join(text_char)\n",
    "        \n",
    "        predicate_position = []\n",
    "\n",
    "        for i, sent in enumerate(nlp(final).sents):\n",
    "\n",
    "            predicate_position.append(c_predicate.findall(sent.text))\n",
    "\n",
    "        positions.append((final, predicate_position))\n",
    "        \n",
    "    return positions\n",
    "    \n",
    "\n",
    "result = []\n",
    "\n",
    "for entry in tqdm(list(train)):\n",
    "    result.append(extract_from_entry(entry))\n",
    "\n",
    "with open('alignment', 'bw') as f:\n",
    "   pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: category=SportsTeam eid=Id128\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Agremiação_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_Série_C\n",
       "Campeonato_Brasileiro_Série_C | country | Brazil\n",
       "Agremiação_Sportiva_Arapiraquense | season | 2015\n",
       "\n",
       "\n",
       "\tLexicalizations:\n",
       "\n",
       "Agremiação Sportiva Arapiraquense play in the Campeonato Brasileiro Série C league in Brazil in 2015.\n",
       "Agremiação Sportiva Arapiraquense play in Brazil's Campeonato Brasileiro Série C league and competed in the 2015 event.\n",
       "Agremiação Sportiva Arapiraquense play in the Brazilian-based Campeonato Brasileiro Série C league and they played in the 2015 season."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(train)\n",
    "X[-600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Agremiação Sportiva Arapiraquense play in the PREDICATE-$league$ league in PREDICATE-$country$ in PREDICATE-$season$.',\n",
       "  [['league', 'country', 'season']]),\n",
       " (\"Agremiação Sportiva Arapiraquense play in PREDICATE-$country$'s PREDICATE-$league$ league and competed in the PREDICATE-$season$ event.\",\n",
       "  [['country', 'league', 'season']]),\n",
       " ('Agremiação Sportiva Arapiraquense play in the Brazilian-based PREDICATE-$league$ league and they played in the PREDICATE-$season$ season.',\n",
       "  [['league', 'season']])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Agremiação Sportiva Arapiraquense play in the PREDICATE-$league$ league in PREDICATE-$country$ in PREDICATE-$season$.',\n",
       "  [['league', 'country', 'season']]),\n",
       " (\"Agremiação Sportiva Arapiraquense play in PREDICATE-$country$'s PREDICATE-$league$ league and competed in the PREDICATE-$season$ event.\",\n",
       "  [['country', 'league', 'season']]),\n",
       " ('Agremiação Sportiva Arapiraquense play in the PREDICATE-$country$-based PREDICATE-$league$ league and they played in the PREDICATE-$season$ season.',\n",
       "  [['country', 'league', 'season']])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_from_entry(X[-600], threshold_abbrev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -500\n",
    "# Gary_Cohn_(comics) -> The comic character Bolt was created by Paris Cullins and Gary Cohn, the former being a United States national.\n",
    "# -600\n",
    "# Brazil -> Brazilian-based\n",
    "# 2\n",
    "# 25.0 -> 25 -> lower the threshold_full?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a predicate ranking recomendation model\n",
    "\n",
    "* todo: analyze the occurrence of different orderings in the same entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter_bigram = Counter()\n",
    "\n",
    "for i, entry_result in enumerate(result):\n",
    "    \n",
    "    for lexe_result in entry_result:\n",
    "        \n",
    "        sentences = lexe_result[1]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            \n",
    "            if len(sentence) > 1:\n",
    "\n",
    "                for i in range(0, len(sentence) - 1):\n",
    "\n",
    "                    counter_bigram[(sentence[i], sentence[i+1])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('country', 'leaderName'), 799),\n",
       " (('region', 'country'), 500),\n",
       " (('location', 'country'), 483),\n",
       " (('location', 'isPartOf'), 421),\n",
       " (('manager', 'club'), 392),\n",
       " (('leaderName', 'leaderName'), 314),\n",
       " (('isPartOf', 'country'), 303),\n",
       " (('country', 'ethnicGroup'), 295),\n",
       " (('club', 'club'), 280),\n",
       " (('leaderName', 'country'), 270)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_bigram.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_sum = Counter()\n",
    "\n",
    "for (p1, _), n in counter_bigram.items():\n",
    "    \n",
    "    counter_sum[p1] += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicates = list(set().union(*[(p1 for (p1, _) in counter_bigram.keys()), (p2 for (_, p2) in counter_bigram.keys())]))\n",
    "\n",
    "from_key_to_int = {k: i for i, k in enumerate(predicates)}\n",
    "from_int_to_key = {i:k for k, i in from_key_to_int.items()}\n",
    "\n",
    "a = np.zeros((len(counter_first_pos), len(counter_first_pos)))\n",
    "\n",
    "for (p1, p2), n in counter_bigram.items():\n",
    "    \n",
    "    a[from_key_to_int[p1], from_key_to_int[p2]] = n / counter_sum[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29625509825732294"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[from_key_to_int['country'], from_key_to_int['leaderName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: category=WrittenWork eid=Id49\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "AIP_Advances | editor | A.T._Charlie_Johnson\n",
       "A.T._Charlie_Johnson | almaMater | Harvard_University\n",
       "A.T._Charlie_Johnson | doctoralAdvisor | Michael_Tinkham\n",
       "\n",
       "\n",
       "\tLexicalizations:\n",
       "\n",
       "A T Charlie Johnson, AIP Advances editor, graduated from Harvard University assisted by doctoral advisor Michael Tinkham.\n",
       "A T Charlie Johnson is the editor AIP Advances. His alma mater was Harvard University and his doctoral adviser was Michael Tinkham."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def get_predicates_sequence(entry):\n",
    "    \n",
    "    predicates = [d['predicate'] for d in entry.get_data()]\n",
    "\n",
    "    for p1, p2 in permutations(predicates, 2):\n",
    "\n",
    "        print(p1, p2)\n",
    "        print(a[from_key_to_int[p1], from_key_to_int[p2]])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: category=University eid=Id7\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "AWH_Engineering_College | country | India\n",
       "AWH_Engineering_College | established | 2001\n",
       "AWH_Engineering_College | city | \"Kuttikkattoor\"\n",
       "\n",
       "\n",
       "\tLexicalizations:\n",
       "\n",
       "AWH Engineering College was established in 2001 in Kuttikkattoor, India.\n",
       "Kuttikkattoor, India is the location of the AWH Engineering College which was established in 2001.\n",
       "The AWH Engineering College was established in 2001 in Kuttikkattoor, India."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country established\n",
      "0.03299962921764924\n",
      "\n",
      "country city\n",
      "0.006674082313681869\n",
      "\n",
      "established country\n",
      "0.09404388714733543\n",
      "\n",
      "established city\n",
      "0.09404388714733543\n",
      "\n",
      "city country\n",
      "0.40394973070017953\n",
      "\n",
      "city established\n",
      "0.08797127468581688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_predicates_sequence(X[-300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AWH Engineering College was established in PREDICATE-$established$ in PREDICATE-$city$ PREDICATE-$country$.',\n",
       "  [['established'], ['city', 'country']]),\n",
       " ('PREDICATE-$city$ PREDICATE-$country$ is the location of the AWH Engineering College which was established in PREDICATE-$established$.',\n",
       "  [['city', 'country', 'established']]),\n",
       " ('The AWH Engineering College was established in PREDICATE-$established$ in PREDICATE-$city$ PREDICATE-$country$.',\n",
       "  [['established'], ['city', 'country']])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
