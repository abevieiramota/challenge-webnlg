{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "\n",
    "from evaluation import evaluate_model\n",
    "\n",
    "from content_selection import SelectAllContentSelector\n",
    "from discourse_structuring import DoesntSortDiscourseStructurer\n",
    "from sentence_aggregation import OneSentenceSentenceAggregator\n",
    "from lexicalization import PreprocessLexicalizer, preprocess_so\n",
    "from sentence_generation import MostFrequentTemplateSentenceGenerator, FallBackPipelineSentenceGenerator, JustJoinTripleSentenceGenerator\n",
    "from text_generation import TextGenerator\n",
    "from data_alignment import NGramDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "\n",
    "import spacy\n",
    "from textacy import similarity\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webnlg_corpus import webnlg\n",
    "\n",
    "corpus = webnlg.load('webnlg_challenge_2017')\n",
    "\n",
    "train_dev = corpus.subset(datasets=['train', 'dev'])\n",
    "\n",
    "train_dev_1 = train_dev.subset(ntriples=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATES_FILE = '../model/ngram_3_levenshtein_templates'\n",
    "\n",
    "if os.path.isfile(TEMPLATES_FILE):\n",
    "    template_db = TemplateExtractor.load(TEMPLATES_FILE)\n",
    "else:\n",
    "    te = TemplateExtractor(\n",
    "            data_alignment_model=NGramDataAlignmentModel(3, similarity.levenshtein, nlp))\n",
    "\n",
    "    template_db = te.extract(train_dev_1)\n",
    "    TemplateExtractor.save('../model/ngram_3_levenshtein_templates')\n",
    "\n",
    "model = TextGenerator(\n",
    "    content_selection_model=SelectAllContentSelector(),\n",
    "    discourse_structuring_model=DoesntSortDiscourseStructurer(),\n",
    "    sentence_aggregation_model=OneSentenceSentenceAggregator(),\n",
    "    lexicalization_model=PreprocessLexicalizer(preprocess=preprocess_so),\n",
    "    sentence_generation_model=FallBackPipelineSentenceGenerator([\n",
    "                                MostFrequentTemplateSentenceGenerator().fit(template_db),\n",
    "                                JustJoinTripleSentenceGenerator()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 31.07, 'meteor': 0.303327810176281, 'ter': 0.6234476919820661}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, '1-predicate-template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f8b2d0c722fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_aggregation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneSentenceAggregator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_generation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJustJoinTripleSentenceGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMostFrequentTemplateSentenceGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdiscourse_structuring\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMostFrequentFirstDiscourseStructuring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChainDiscourseStructuring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_alignment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRootDataAlignmentModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNGramDataAlignmentModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSPODataAlignmentModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFallBackDataAlignmentModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtemplate_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemplateExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\challenge-webnlg\\script\\discourse_structuring.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "\n",
    "from sentence_generation import FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from sentence_aggregation import OneSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned,LexicalizePreprocessed\n",
    "from webnlg import preprocess_triple_text\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textacy import similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Alignment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 16s, sys: 8min 14s, total: 27min 31s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "rda = RootDataAlignmentModel(similarity.levenshtein, nlp)\n",
    "ngramda = NGramDataAlignmentModel(3, similarity.levenshtein, nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da1 = FallBackDataAlignmentModel(models=[ngramda, rda])\n",
    "da2 = FallBackDataAlignmentModel(models=[rda, ngramda])\n",
    "\n",
    "\n",
    "das = [da1, da2]\n",
    "\n",
    "tes = []\n",
    "\n",
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    filepath = f'{MODEL_DIR}{i}'\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        te = TemplateExtractor.load(filepath)\n",
    "    else:\n",
    "        te = TemplateExtractor(da)\n",
    "\n",
    "        # texts from entries\n",
    "        texts = train_dev_1.ldf.ltext.tolist()\n",
    "\n",
    "        # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "        #    will be only one element)\n",
    "\n",
    "        # for each entry you have one or more verbalizations\n",
    "        #    you have to repeat the tripleset N times, N = number of verbalizations\n",
    "        datas = chain.from_iterable([\\\n",
    "            # data for entry\n",
    "            [entry.get_data()[0]] \\\n",
    "                 * \\\n",
    "            # number of verbalizations\n",
    "            entry.ldf.shape[0] for entry in train_dev_1])\n",
    "\n",
    "        te.fit(texts, datas)\n",
    "\n",
    "        TemplateExtractor.save(te, filepath)\n",
    "        \n",
    "    tes.append(te)\n",
    "    \n",
    "    \n",
    "    # uses the most frequente template\n",
    "    mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "    # uses the nearest predicate templates\n",
    "    #    precalculate nearests for test_not_in_1\n",
    "    npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                    similarity_metric=similarity.levenshtein,\n",
    "                                                    predicates=test_not_in_1,\n",
    "                                                    preprocessor=preprocess_triple_text,\n",
    "                                                    threshold=threshold)\n",
    "    # baseline\n",
    "    jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "    sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])\n",
    "\n",
    "    text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "    #mff = MostFrequentFirstDiscourseStructuring(template_model=te)\n",
    "    # Starts from a node without incoming vertices\n",
    "    #    and then do a Breadth first search\n",
    "    cds = ChainDiscourseStructuring()\n",
    "\n",
    "    le = LexicalizeAsAligned(da)\n",
    "    lp = LexicalizePreprocessed()\n",
    "\n",
    "    def replace_subject(d):\n",
    "\n",
    "        d['m_subject'] = ','\n",
    "\n",
    "        return d\n",
    "\n",
    "    # starting from second sentence, apply replace_subject function over data :)\n",
    "\n",
    "    #pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, le, processor=replace_subject, nth=0)\n",
    "    pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, lp, processor=replace_subject, nth=0)\n",
    "\n",
    "    import codecs\n",
    "\n",
    "    with codecs.open(filepath, 'w', 'utf-8') as f:\n",
    "\n",
    "        for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "\n",
    "            f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c0\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c1\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c2\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c3\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c4\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c5\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c6\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c7\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c8\n"
     ]
    }
   ],
   "source": [
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    filepath = f'{MODEL_DIR}{i}'\n",
    "    model_namee = f'{model_name}{i}'\n",
    "\n",
    "    !python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_namee\" --team_filepath \"$filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 36.97, 70.8/45.1/29.9/19.6 (BP=1.000, ratio=1.138, hyp_len=52032, ref_len=45719)\n",
      "BLEU = 38.57, 73.4/47.3/31.2/20.5 (BP=1.000, ratio=1.097, hyp_len=49259, ref_len=44910)\n",
      "BLEU = 38.94, 74.1/47.8/31.4/20.7 (BP=1.000, ratio=1.085, hyp_len=48519, ref_len=44714)\n",
      "BLEU = 32.02, 65.7/39.7/25.3/16.0 (BP=1.000, ratio=1.175, hyp_len=53718, ref_len=45703)\n",
      "BLEU = 33.33, 68.4/41.6/26.2/16.6 (BP=1.000, ratio=1.129, hyp_len=50663, ref_len=44889)\n",
      "BLEU = 33.80, 69.2/42.2/26.5/16.9 (BP=1.000, ratio=1.113, hyp_len=49718, ref_len=44672)\n",
      "BLEU = 39.40, 74.6/48.1/32.1/20.9 (BP=1.000, ratio=1.061, hyp_len=47444, ref_len=44702)\n",
      "BLEU = 40.52, 76.6/49.8/32.9/21.5 (BP=1.000, ratio=1.030, hyp_len=45505, ref_len=44163)\n",
      "BLEU = 40.84, 77.2/50.2/33.1/21.7 (BP=1.000, ratio=1.021, hyp_len=44946, ref_len=44016)\n"
     ]
    }
   ],
   "source": [
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    bleu_all_cat_filepath = os.path.join(model_temp_dir, f\"{model_name}{i}_all-cat.txt\")\n",
    "\n",
    "    !../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat_filepath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rda = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "ngramda = NGramDataAlignmentModel(4, similarity.levenshtein, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da = FallBackDataAlignmentModel(models=[ngramda, spoda, rda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 1min 14s, total: 3min 21s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    te = TemplateExtractor.load(MODEL_DIR)\n",
    "else:\n",
    "    te = TemplateExtractor(da)\n",
    "\n",
    "    # texts from entries\n",
    "    texts = train_dev_1.ldf.ltext.tolist()\n",
    "\n",
    "    # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "    #    will be only one element)\n",
    "\n",
    "    # for each entry you have one or more verbalizations\n",
    "    #    you have to repeat the tripleset N times, N = number of verbalizations\n",
    "    datas = chain.from_iterable([\\\n",
    "        # data for entry\n",
    "        [entry.get_data()[0]] \\\n",
    "             * \\\n",
    "        # number of verbalizations\n",
    "        entry.ldf.shape[0] for entry in train_dev_1])\n",
    "\n",
    "    te.fit(texts, datas)\n",
    "\n",
    "    TemplateExtractor.save(te, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the most frequente template\n",
    "mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "# uses the nearest predicate templates\n",
    "#    precalculate nearests for test_not_in_1\n",
    "npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                similarity_metric=similarity.levenshtein,\n",
    "                                                predicates=test_not_in_1,\n",
    "                                                preprocessor=preprocess_triple_text,\n",
    "                                                threshold=.7)\n",
    "# baseline\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence aggregation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse structuring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mff = MostFrequentFirstDiscourseStructuring(template_model=te)\n",
    "# Starts from a node without incoming vertices\n",
    "#    and then do a Breadth first search\n",
    "cds = ChainDiscourseStructuring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicalization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LexicalizeAsAligned(da)\n",
    "lp = LexicalizePreprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject(d):\n",
    "    \n",
    "    d['m_subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "# starting from second sentence, apply replace_subject function over data :)\n",
    "\n",
    "#pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, le, processor=replace_subject, nth=0)\n",
    "pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, lp, processor=replace_subject, nth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'Airport', 'eid': 'Id4', 'idx': '0_3', 'ntriples': 1}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Afonso_Pena_International_Airport | ICAO_Location_Identifier | \"SBCT\"\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = test.sample(idx='0_3')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ICAO Location Identifier of Afonso Pena International Airport is SBCT.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.generate([e.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_subject': 'Afonso Pena International Airport',\n",
       " 'm_predicate': 'ICAO_Location_Identifier',\n",
       " 'm_object': '\"SBCT\"'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lexicalize(e.get_data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total area of Atlantic City, New Jersey is square kilometres 44.125.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = test.sample()\n",
    "pipe.generate([sample.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'City', 'eid': 'Id172', 'idx': '0_171', 'ntriples': 1}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Atlantic_City,_New_Jersey | areaTotal | 44.125 (square kilometres)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.3 s, sys: 266 ms, total: 6.56 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import codecs\n",
    "\n",
    "with codecs.open(output_filepath, 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English language is spoken in novel Castle.\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Macmillan Publishers is the parent company of Farrar, Straus and Giroux.\r\n",
      "One of John Cowper Powys notable works is Oliver A Glastonbury Romance.\r\n",
      "Soho Press is in the United States.\r\n",
      "The Secret Scripture was published by Faber and Faber.\r\n",
      "Asian Americans are an ethnic group in the United States.\r\n",
      "The English language is spoken in United States.\r\n",
      "Weymouth Sands was preceded by A Glastonbury Romance.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 \"$output_filepath\" | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_c952c4774651e01f72450b3ac8ebcde98ab9f157\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_name\" --team_filepath \"$output_filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 39.89, 75.3/49.0/32.3/21.2 (BP=1.000, ratio=1.068, hyp_len=47712, ref_len=44677)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
