{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "\n",
    "import webnlg\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = webnlg.load(dataset=['train', 'dev', 'test_with_lex'], structure='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number of entries                             9674\n",
       "number of distinct triples                    3221\n",
       "number of distinct generated texts           25214\n",
       "number of reference texts                    25298\n",
       "number of triples                            28399\n",
       "number of characters in reference texts    2956533\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_stats = pd.Series({\n",
    "    'number of entries': len(corpus.edf),\n",
    "    'number of distinct triples': corpus.mdf.mtext.nunique(),\n",
    "    'number of distinct generated texts': corpus.ldf.ltext.nunique(),\n",
    "    'number of reference texts': len(corpus.ldf),\n",
    "    'number of triples': len(corpus.mdf),\n",
    "    'number of characters in reference texts': corpus.ldf.ltext.str.len().sum()\n",
    "    })\n",
    "\n",
    "datasets_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One entry per category and triplesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_entries = corpus.edf.groupby(['category', 'ntriples']).apply(lambda g: g.sample(random_state=10))\n",
    "\n",
    "len(random_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_entries_with_lexes = pd.merge(random_entries, corpus.ldf, on='idx')\n",
    "\n",
    "len(random_entries_with_lexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_entries_with_lexes['Google Translate'] = None\n",
    "\n",
    "\n",
    "random_entries_with_lexes.sort_values(['category', 'ntriples']).to_excel('sample_translation_evaluation.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.merge(corpus.edf, corpus.ldf, on='idx')\n",
    "\n",
    "all_texts['Google Translate'] = None\n",
    "\n",
    "all_texts.sort_values(['category', 'ntriples'], inplace=True)\n",
    "\n",
    "all_texts.to_excel('all_texts_translation_evaluation.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to use google translate, I have to split the texts in buckets of 30k characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "buckets_dir = './all_texts_buckets'\n",
    "\n",
    "if not os.path.isdir(buckets_dir):\n",
    "    \n",
    "    os.mkdir(buckets_dir)\n",
    "\n",
    "n_characters = 30000\n",
    "\n",
    "accumulated = 0\n",
    "\n",
    "positions = []\n",
    "\n",
    "for i, (idx, text_len) in enumerate(all_texts['ltext'].str.len().iteritems()):\n",
    "    \n",
    "    accumulated += text_len\n",
    "    \n",
    "    if accumulated > n_characters:\n",
    "        \n",
    "        positions.append(i - 1)\n",
    "        accumulated = text_len\n",
    "        \n",
    "for i, (i_begin, i_end) in enumerate(zip([0] + positions, positions + [None])):\n",
    "    \n",
    "    bucket = all_texts.iloc[i_begin:i_end, :]\n",
    "    \n",
    "    bucket['ltext'].to_csv(os.path.join(buckets_dir, f'all_texts_bucket_{i}.txt'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After translating all the texts, lets bring them together into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('all_texts_buckets/Todos textos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_texts_buckets/translations_0_21.txt',\n",
       " 'all_texts_buckets/translations_21_30.txt',\n",
       " 'all_texts_buckets/translations_31_40.txt',\n",
       " 'all_texts_buckets/translations_41_50.txt',\n",
       " 'all_texts_buckets/translations_51_60.txt',\n",
       " 'all_texts_buckets/translations_61_70.txt',\n",
       " 'all_texts_buckets/translations_71_80.txt',\n",
       " 'all_texts_buckets/translations_81_90.txt',\n",
       " 'all_texts_buckets/translations_91_98.txt']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "translation_files = glob('all_texts_buckets/translations_*.txt')\n",
    "\n",
    "translation_files.sort()\n",
    "\n",
    "translation_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "\n",
    "for translation_file in translation_files:\n",
    "    \n",
    "    with open(translation_file) as f:\n",
    "        \n",
    "        translations.extend(f.readlines())\n",
    "\n",
    "translations = [t.strip() for t in translations]\n",
    "df['Google Translate'] = translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_dataset = {dataset: group for dataset, group in df.groupby('dataset')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FOLDER = 'corpus_pt'\n",
    "OUTFILE_TEMPLATE = '{}.xml'\n",
    "\n",
    "LEX_XML_ELEMENT_TEMPLATE = '<lex comment=\"none\" lid=\"{lid}\">{Google Translate}</lex>'\n",
    "\n",
    "C_LEX_ELEMENT = re.compile(r'<lex.*</lex>', flags=re.DOTALL)\n",
    "\n",
    "def replace_lex_by_slot(xml):\n",
    "    \n",
    "    return C_LEX_ELEMENT.sub('{lexes}', xml)\n",
    "\n",
    "if not os.path.isdir(CORPUS_FOLDER):\n",
    "    \n",
    "    os.mkdir(CORPUS_FOLDER)\n",
    "    \n",
    "i = 0\n",
    "\n",
    "for dataset, df in df_by_dataset.items():\n",
    "    \n",
    "    outfile = os.path.join(CORPUS_FOLDER, OUTFILE_TEMPLATE.format(dataset))\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        \n",
    "        f.write('<benchmark>\\n')\n",
    "        f.write('\\t<entries>\\n\\t\\t')\n",
    "        \n",
    "        for idx, df_rows in df.groupby('idx'):\n",
    "            \n",
    "            xml = replace_lex_by_slot(df_rows.iloc[0]['content'])\n",
    "            \n",
    "            lex_xml_elements = []\n",
    "            \n",
    "            for i, row in df_rows.iterrows():\n",
    "                \n",
    "                lex_xml_element = LEX_XML_ELEMENT_TEMPLATE.format(**row.to_dict())\n",
    "                \n",
    "                lex_xml_elements.append(lex_xml_element)\n",
    "                \n",
    "            xml = xml.format(**{'lexes': '\\n\\t\\t\\t'.join(lex_xml_elements)})\n",
    "            \n",
    "            f.write(xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
