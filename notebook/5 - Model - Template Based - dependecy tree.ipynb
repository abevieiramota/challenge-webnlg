{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by_groupname(m):\n",
    "    \n",
    "    return \"{{{}}}\".format(next((k for k, v in m.groupdict().items() if v)))\n",
    "\n",
    "DISTANCE_THRESHOLD = 200\n",
    "\n",
    "#TODO: parameterize distance metric\n",
    "nnc_logger = logging.getLogger(\"nearest_noun_chunks\")\n",
    "def nearest_noun_chunks(doc, m_subject, m_object):\n",
    "    \n",
    "    map_text_into_function = {m_subject: 'm_subject',\n",
    "                              m_object: 'm_object'}\n",
    "    \n",
    "    distances_s = [(m_subject, nc, edit_distance(m_subject, nc.text)) for nc in doc.noun_chunks]\n",
    "    distances_o = [(m_object, nc, edit_distance(m_object, nc.text)) for nc in doc.noun_chunks]\n",
    "    \n",
    "    distances = distances_s + distances_o\n",
    "    \n",
    "    if not distances:\n",
    "        \n",
    "        raise Exception(\"doc without sufficient noun chunks: {}\".format(doc))\n",
    "    \n",
    "    nnc_logger.debug(distances)\n",
    "    \n",
    "    min_distance_1 = min(distances, key=lambda v: v[2])\n",
    "    \n",
    "    if min_distance_1[2] > DISTANCE_THRESHOLD:\n",
    "        nnc_logger.warning(\"distance threshold: {}\".format(min_distance_1))\n",
    "    \n",
    "    # remove distances from already matched resource\n",
    "    distances_without_m = [v for v in distances if v[1] != min_distance_1[1] and v[0] != min_distance_1[0]]\n",
    "    \n",
    "    if not distances_without_m:\n",
    "        \n",
    "        raise Exception(\"doc without sufficient noun chunks: {}\".format(doc))\n",
    "    \n",
    "    nnc_logger.debug(distances_without_m)\n",
    "    \n",
    "    min_distance_2 = min(distances_without_m, key=lambda v: v[2])\n",
    "    \n",
    "    if min_distance_2[2] > DISTANCE_THRESHOLD:\n",
    "        nnc_logger.warning(\"distance threshold: {}\".format(min_distance_2))\n",
    "\n",
    "    return {map_text_into_function[min_distance_1[0]]: min_distance_1[1],\n",
    "            map_text_into_function[min_distance_2[0]]: min_distance_2[1]}\n",
    "\n",
    "\n",
    "class TemplateExtractor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.logger = logging.getLogger('TemplateExtractor')\n",
    "    \n",
    "    def extract_template(self, text, triple):\n",
    "        \n",
    "        slots = {}\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        ncc = nearest_noun_chunks(doc, triple['m_subject'], triple['m_object'])\n",
    "        ncc_regex_escaped = {k: re.escape(v.text) for k, v in ncc.items()}\n",
    "        \n",
    "        self.logger.debug(ncc)\n",
    "        \n",
    "        # is it necessary to compile?\n",
    "        c = re.compile(\"((?P<m_subject>{m_subject})|(?P<m_object>{m_object}))\".format(**ncc_regex_escaped))\n",
    "\n",
    "        return Template(c.sub(replace_by_groupname, doc.text), text, triple)\n",
    "    \n",
    "\n",
    "#TODO: search python template libraries\n",
    "class Template(object):\n",
    "    \n",
    "    def __init__(self, template_string, text, triple):\n",
    "        \n",
    "        self.template_string = template_string\n",
    "        self.triple = triple\n",
    "        self.text = text\n",
    "        \n",
    "    def fill(self, triple):\n",
    "        \n",
    "        return self.template_string.format(**triple)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        return self.template_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TemplateExtractor()\n",
    "\n",
    "t = te.extract_template(\"Eleanor Rigby picks up the rice in the church\",\n",
    "                        {'m_subject': 'Eleanor Rigby', \n",
    "                         'm_predicate': 'pick up', \n",
    "                         'm_object': 'church'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abelardo picks up the rice in car'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.fill({'m_subject': 'Abelardo',\n",
    "        'm_predicate': 'drive',\n",
    "        'm_object': 'car'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../script/webnlg.py\n",
    "\n",
    "train = WebNLGCorpus.load('train')\n",
    "train_1 = train.subset(ntriples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 1.59 s, total: 2min 19s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "logger.setLevel(logging.ERROR)\n",
    "template_db = defaultdict(set)\n",
    "\n",
    "lexes_triples = pd.merge(train_1.ldf, train_1.mdf)\n",
    "\n",
    "te = TemplateExtractor()\n",
    "\n",
    "for ix, row in lexes_triples.iterrows():\n",
    "    lexe = row['ltext']\n",
    "    triple = {'m_subject': row['m_subject'],\n",
    "              'm_object': row['m_object'],\n",
    "              'm_predicate': row['m_predicate']\n",
    "             }\n",
    "    try:\n",
    "        t = te.extract_template(lexe, triple)\n",
    "    \n",
    "        template_db[row['m_predicate']].add(t)\n",
    "    except Exception as ex:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1.mdf.m_predicate.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    225.000000\n",
       "mean      18.613333\n",
       "std       36.900648\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        6.000000\n",
       "75%       18.000000\n",
       "max      290.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index = [(m_predicate, len(templates)) for m_predicate, templates in template_db.items()]\n",
    "data = [d[1] for d in data_index]\n",
    "index = [d[0] for d in data_index]\n",
    "\n",
    "stats_on_templates = pd.Series(data=data, index=index)\n",
    "stats_on_templates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country         290\n",
       "isPartOf        235\n",
       "leaderName      219\n",
       "location        193\n",
       "club            168\n",
       "ingredient      131\n",
       "language        124\n",
       "runwayLength    106\n",
       "creator          94\n",
       "ethnicGroup      74\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_on_templates.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Atlantic City International Airport serves Atlantic City, N.J.\n",
      "\n",
      "{'m_subject': 'Atlantic_City_International_Airport', 'm_object': 'Atlantic_City,_New_Jersey', 'm_predicate': 'cityServed'}\n",
      "\n",
      "{m_subject} serves {m_object}, N.J.\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "i = 1\n",
    "t = list(islice(template_db['cityServed'], i, i+1))[0]\n",
    "\n",
    "print(t.text)\n",
    "print()\n",
    "print(t.triple)\n",
    "print()\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
