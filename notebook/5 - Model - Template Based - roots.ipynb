{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../script/webnlg.py\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from textacy import similarity\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(doc, node):\n",
    "\n",
    "    return doc[node.left_edge.i: node.right_edge.i + 1]\n",
    "\n",
    "def get_left_span(doc, node):\n",
    "\n",
    "    return doc[node.left_edge.i: node.i + 1]\n",
    "\n",
    "def get_right_span(doc, node):\n",
    "\n",
    "    return doc[node.i: node.right_edge.i + 1]\n",
    "\n",
    "def as_span(doc, node):\n",
    "\n",
    "    return doc[node.i: node.i + 1]\n",
    "\n",
    "class RootDataAlignmentModel:\n",
    "    \n",
    "    def __init__(self, similarity_metric):\n",
    "        \n",
    "        self.similarity_metric = similarity_metric\n",
    "        \n",
    "\n",
    "    def get_distances(self, doc, data):\n",
    "\n",
    "        distances, spans = [], []\n",
    "\n",
    "        # dependency trees' roots\n",
    "        roots = [token for token in doc if token.head == token]\n",
    "\n",
    "        # BIAS: parts of the tree\n",
    "        # breadth-first\n",
    "        for root in roots:\n",
    "\n",
    "            # root subtree\n",
    "            # BIAS: parts of the tree\n",
    "            root_span = get_span(doc, root)\n",
    "            # root left subtree\n",
    "            root_left_span = get_left_span(doc, root)\n",
    "            # root right subtree\n",
    "            root_right_span = get_right_span(doc, root)\n",
    "            # root node\n",
    "            only_root = as_span(doc, root)\n",
    "\n",
    "            # test agains the node and its subtree\n",
    "            for span in set((only_root, root_span, root_left_span, root_right_span)):\n",
    "\n",
    "                spans.append(span)\n",
    "\n",
    "                distances_span = []\n",
    "\n",
    "                # for each structured data, calculate similarity\n",
    "                for d in data.values():\n",
    "\n",
    "                    distances_span.append(self.similarity_metric(d, span.text))\n",
    "\n",
    "                distances.append(distances_span)\n",
    "            \n",
    "            # add children\n",
    "            roots.extend(root.lefts)\n",
    "            roots.extend(root.rights)\n",
    "\n",
    "        return pd.DataFrame(distances, index=spans, columns=data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests if two spans overlaps\n",
    "# TODO: move to spacy_util\n",
    "def overlaps(span1, span2):\n",
    "\n",
    "    return max(0, min(span1.end_char, span2.end_char) - max(span1.start_char, span2.start_char)) > 0\n",
    "\n",
    "\n",
    "class TemplateExtractionModel:\n",
    "    \n",
    "    def __init__(self, data_alignment_model):\n",
    "    \n",
    "        self.data_alignment_model = data_alignment_model\n",
    "        \n",
    "\n",
    "    def extract_template(self, text, data):\n",
    "\n",
    "        # spacy model\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # use data alignment model to get similarity matrix\n",
    "        df = self.data_alignment_model.get_distances(doc, data)\n",
    "\n",
    "        # breaks text into char array\n",
    "        text_char = list(text)\n",
    "\n",
    "        # subject extraction\n",
    "        # BIAS: subject wins priority over distances tie\n",
    "        m_subject_span = df.m_subject.nlargest(1).index.values[0]\n",
    "\n",
    "        # replaces subject text with m_subject placeholder\n",
    "        text_char[m_subject_span.start_char: m_subject_span.end_char] = '{m_subject}'\n",
    "\n",
    "        # object extraction\n",
    "        # search for the best span for object, different from the subject one\n",
    "        for span in df.m_object.sort_values(ascending=False).index.values:\n",
    "\n",
    "            # tests if the current span doesn't overlaps the subject one\n",
    "            if overlaps(span, m_subject_span):\n",
    "\n",
    "                continue\n",
    "\n",
    "            # tests if the object occurs after the subject >\n",
    "            #    if it is the case, you have to adjust the indexes accordingly\n",
    "            if m_subject_span.start_char > span.end_char:\n",
    "\n",
    "                base = 0\n",
    "            else:\n",
    "                # adjustes the indexes\n",
    "                # length of the extracted subject text\n",
    "                len_subject_text = m_subject_span.end_char - m_subject_span.start_char\n",
    "                # length of the placeholder minus len_subject_text\n",
    "                base = len('{m_subject}') - len_subject_text\n",
    "\n",
    "            # replaces object text with m_object placeholder\n",
    "            text_char[base + span.start_char: base + span.end_char] = '{m_object}'\n",
    "\n",
    "            break\n",
    "\n",
    "        # build template using the char array\n",
    "        return ''.join(text_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = WebNLGCorpus.load('train')\n",
    "\n",
    "# BIAS: use only 1 tripleset size dataset\n",
    "train_1 = train.subset(ntriples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Faroese language is spoken in Denmark.',\n",
       " {'m_subject': 'Denmark', 'm_object': 'Faroese_language'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an entry\n",
    "e = train_1.sample(idx='0_210')\n",
    "\n",
    "# data alignment model with token_sort_ratio similarity metric\n",
    "# PARAM/BIAS: similarity_metric \n",
    "# TODO: train changing similarity_metric\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "# template extraction model\n",
    "te = TemplateExtractionModel(da)\n",
    "    \n",
    "# uses the first reference text\n",
    "# it can have more than 1 reference text\n",
    "first_lexicalization = e.ldf.ltext.values.tolist()[0]\n",
    "# uses the first triple\n",
    "first_triple = e.mdf[['m_subject', 'm_object']].to_dict(orient='records')[0]\n",
    "\n",
    "first_lexicalization, first_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{m_object} is spoken in {m_subject}.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracts the template\n",
    "template = te.extract_template(first_lexicalization, first_triple)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Portuguese is spoken in Brazil.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.format(**{'m_subject': 'Brazil', 'm_object': 'Portuguese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{m_subject} picks up the {m_object} in the church\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Abelardo Vieira Mota picks up the car in the church'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove lexicalized information not present in data\n",
    "text = 'Eleanor Rigby picks up the rice in the church'\n",
    "data = {'m_subject': 'Eleanor Rigby', \n",
    "        'm_object': 'rice'}\n",
    "\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "te = TemplateExtractionModel(da)\n",
    "\n",
    "template = te.extract_template(text, data)\n",
    "\n",
    "print(template)\n",
    "\n",
    "template.format(**{'m_subject': 'Abelardo Vieira Mota', 'm_object': 'car'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 25s, sys: 9.26 s, total: 3min 34s\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "te = TemplateExtractionModel(da)\n",
    "\n",
    "template_db = defaultdict(list)\n",
    "\n",
    "#! BIAS: using only train_1 sentences\n",
    "# for each sentence, extracts template\n",
    "for entry in train_1:\n",
    "    \n",
    "    for text in entry.ldf.ltext.tolist():\n",
    "        # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "        #    will be only one element)\n",
    "        data = entry.mdf[['m_subject', 'm_object']].to_dict(orient='records')[0]\n",
    "        predicate = entry.mdf.m_predicate.values[0]\n",
    "\n",
    "        template = te.extract_template(text, data)\n",
    "\n",
    "        # add to db\n",
    "        template_db[predicate].append((entry.edf.idx.values[0], template))\n",
    "    \n",
    "# most frequent template\n",
    "for k, templates in template_db.items():\n",
    "    \n",
    "    template_db[k] = Counter([v[1] for v in templates]).most_common(1)[0][0]\n",
    "    \n",
    "predicates_in_db = list(template_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{m_object} is spoken in {m_subject}.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_db['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do I have one predicate for each predicate in test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WebNLGCorpus.load('test_no_lex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_in_test = set(test.mdf.m_predicate.tolist())\n",
    "predicates_w_template = template_db.keys()\n",
    "\n",
    "len(\"There are {} predicates in test which don't have a template\".format(len(predicates_in_test ^ predicates_w_template)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, let them fall back to the nearest predicate and then to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.Logger('TemplateBasedModel')\n",
    "\n",
    "unwanted_separators = re.compile(r'(\\||_)')\n",
    "unwanted_multiple_empty = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_triple(s):\n",
    "    \n",
    "    sep_changed = unwanted_separators.sub(' ', s)\n",
    "    mult_empty_removed = unwanted_multiple_empty.sub(' ', sep_changed)\n",
    "    \n",
    "    return mult_empty_removed.replace('\"', '')\n",
    "\n",
    "def get_nearest_predicate(predicate):\n",
    "    \n",
    "    distances = [(in_db, similarity.jaro_winkler(predicate, in_db)) for in_db in predicates_in_db]\n",
    "    \n",
    "    return min(distances, key=lambda v: v[1])\n",
    "\n",
    "\n",
    "def generate_sentences(entry):\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    for i, triple in entry.mdf.iterrows():\n",
    "\n",
    "        m_predicate = triple.m_predicate\n",
    "\n",
    "        if m_predicate in template_db:\n",
    "            \n",
    "            template = template_db[m_predicate]\n",
    "\n",
    "            preprocessed_triple = triple[['m_subject', 'm_object']].apply(preprocess_triple)\n",
    "\n",
    "            text = template.format(**preprocessed_triple.to_dict())\n",
    "\n",
    "        else:\n",
    "            \n",
    "            nearest_predicate, similarity = get_nearest_predicate(m_predicate)\n",
    "            \n",
    "            if similarity > .4:\n",
    "                \n",
    "                logger.warning(\"Fallback nearest predicate for predicate %s\", m_predicate)\n",
    "                \n",
    "                template = template_db[nearest_predicate]\n",
    "\n",
    "                preprocessed_triple = triple[['m_subject', 'm_object']].apply(preprocess_triple)\n",
    "\n",
    "                text = template.format(**preprocessed_triple.to_dict())\n",
    "                \n",
    "            else:\n",
    "                \n",
    "               # logger.warning(\"Fallback baseline for predicate %s\", m_predicate)\n",
    "\n",
    "                text = preprocess_triple(triple.mtext)\n",
    "\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>mtext</th>\n",
       "      <th>m_subject</th>\n",
       "      <th>m_predicate</th>\n",
       "      <th>m_object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | ISSN_number | \"0567-7920\"</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>ISSN_number</td>\n",
       "      <td>\"0567-7920\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | LCCN_number | 60040714</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>LCCN_number</td>\n",
       "      <td>60040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | abbreviation | \"Acta Palaeontol. Pol.\"</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>abbreviation</td>\n",
       "      <td>\"Acta Palaeontol. Pol.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  \\\n",
       "639  0_419   \n",
       "640  0_419   \n",
       "641  0_419   \n",
       "\n",
       "                                                                      mtext  \\\n",
       "639               Acta_Palaeontologica_Polonica | ISSN_number | \"0567-7920\"   \n",
       "640                  Acta_Palaeontologica_Polonica | LCCN_number | 60040714   \n",
       "641  Acta_Palaeontologica_Polonica | abbreviation | \"Acta Palaeontol. Pol.\"   \n",
       "\n",
       "                         m_subject   m_predicate                 m_object  \n",
       "639  Acta_Palaeontologica_Polonica   ISSN_number              \"0567-7920\"  \n",
       "640  Acta_Palaeontologica_Polonica   LCCN_number                 60040714  \n",
       "641  Acta_Palaeontologica_Polonica  abbreviation  \"Acta Palaeontol. Pol.\"  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = test.sample(idx='0_419')\n",
    "\n",
    "test_sample.mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The ISSN number of Acta Palaeontologica Polonica is 0567-7920.',\n",
       " 'The LCCN number of Acta Palaeontologica Polonica is 60040714.',\n",
       " 'Acta Palaeontologica Polonica is abbreviated Acta Palaeontol. Pol.. Pol.']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "generate_sentences(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating texts for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "with codecs.open('models/template_based/output.txt', 'w', 'utf-8') as f:\n",
    "    \n",
    "    for entry in test:\n",
    "        \n",
    "        entry_sentences = generate_sentences(entry)\n",
    "        \n",
    "        entry_text = ' '.join(entry_sentences)\n",
    "        \n",
    "        f.write(entry_text)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language is spoken in Castle (novel).\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Farrar, Straus and Giroux is the parent company of the Macmillan Publishers Press.\r\n",
      "One of John Cowper Powys notable works is Oliver A Glastonbury Romance.\r\n",
      "Soho Press is located in United States.\r\n",
      "Faber and Faber is the publisher of The Secret Scripture.\r\n",
      "Asian Americans are an ethnic group in the United United States.\r\n",
      "English language is spoken in United States.\r\n",
      "Weymouth Sands by 1634: The A Glastonbury Romance Affair.\r\n",
      "Rolando Maran manages A.C. Chievo Verona.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 models/template_based/output.txt | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  template_based\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name template_based --team_filepath models/template_based/output.txt --outdir models/template_based/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 37.01, 65.0/44.8/30.7/21.0 (BP=1.000, ratio=1.156, hyp_len=52216, ref_len=45189)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < models/template_based/template_based_all-cat.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
