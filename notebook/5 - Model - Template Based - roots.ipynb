{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets notebook name and commit hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = nb_name.rsplit('.')[0]\n",
    "\n",
    "commit = !git rev-parse HEAD\n",
    "commit = commit[0]\n",
    "\n",
    "model_name = \"{}_{}\".format(nb_name, commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"{}.txt\".format(model_name)\n",
    "model_filename = \"{}\".format(model_name)\n",
    "log_filename = \"{}.log\".format(model_name)\n",
    "\n",
    "import os\n",
    "\n",
    "output_filepath = os.path.join('../data/models', output_filename)\n",
    "model_filepath = os.path.join('../data/models', model_filename)\n",
    "log_filepath = os.path.join('../data/models', log_filename)\n",
    "\n",
    "model_temp_dir = os.path.join('../tmp/', model_name)\n",
    "\n",
    "bleu_all_cat = os.path.join(model_temp_dir, \"{}_all-cat.txt\".format(model_name))\n",
    "\n",
    "if not os.path.isdir('../tmp'):\n",
    "    os.mkdir('../tmp')\n",
    "    \n",
    "if not os.path.isdir(model_temp_dir):\n",
    "    os.mkdir(model_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 - Model - Template Based - roots_9c17c33ef5d01be49d733d80c2fca2e8708bc9e2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, \n",
    "                    level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from webnlg import WebNLGCorpus\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "train_dev = WebNLGCorpus.load(['train', 'dev'])\n",
    "test = WebNLGCorpus.load('test_no_lex')\n",
    "\n",
    "# BIAS: use only 1 tripleset size dataset\n",
    "train_dev_1 = train_dev.subset(ntriples=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in train+dev and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 m_predicates in train+dev not present in train_1+dev_1.\n",
      "They are:\n",
      "\n",
      "gemstone\n",
      "served\n",
      "has to its southeast\n",
      "has to its northwest\n",
      "numberOfRooms\n",
      "5th_runway_SurfaceType\n",
      "architecture\n",
      "neighboringMunicipality\n",
      "servingSize\n"
     ]
    }
   ],
   "source": [
    "train_dev_1_predicates = set(train_dev_1.mdf.m_predicate.unique())\n",
    "train_dev_predicates = set(train_dev.mdf.m_predicate.unique())\n",
    "\n",
    "all_not_in_1 = train_dev_predicates.difference(train_dev_1_predicates)\n",
    "\n",
    "print(\"There are {} m_predicates in train+dev not present in train_1+dev_1.\\nThey are:\\n\\n{}\".format(\n",
    "      len(all_not_in_1), '\\n'.join(all_not_in_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in test and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 117 predicates in test, from 300, which don't have a template\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_in_test = set(test.mdf.m_predicate.unique())\n",
    "test_not_in_1 = predicates_in_test.difference(train_dev_1_predicates)\n",
    "\n",
    "\"There are {} predicates in test, from {}, which don't have a template\".format(len(test_not_in_1),\n",
    "                                                                               len(predicates_in_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the predicate doesn't exist, fall back to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_generation import FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned,LexicalizePreprocessed\n",
    "from webnlg import preprocess_triple_text\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textacy import similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Alignment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rda = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "ngramda = NGramDataAlignmentModel(4, similarity.levenshtein, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da = FallBackDataAlignmentModel(models=[ngramda, spoda, rda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 31.2 ms, total: 31.2 ms\n",
      "Wall time: 32.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if os.path.exists(model_filepath):\n",
    "    te = TemplateExtractor.load(model_filepath)\n",
    "else:\n",
    "    te = TemplateExtractor(da)\n",
    "\n",
    "    # texts from entries\n",
    "    texts = train_dev_1.ldf.ltext.tolist()\n",
    "\n",
    "    # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "    #    will be only one element)\n",
    "\n",
    "    # for each entry you have one or more verbalizations\n",
    "    #    you have to repeat the tripleset N times, N = number of verbalizations\n",
    "    datas = chain.from_iterable([\\\n",
    "        # data for entry\n",
    "        [entry.get_data()[0]] \\\n",
    "             * \\\n",
    "        # number of verbalizations\n",
    "        entry.ldf.shape[0] for entry in train_dev_1])\n",
    "\n",
    "    te.fit(texts, datas)\n",
    "\n",
    "    TemplateExtractor.save(te, model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the most frequente template\n",
    "mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "# uses the nearest predicate templates\n",
    "#    precalculate nearests for test_not_in_1\n",
    "npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                similarity_metric=similarity.levenshtein,\n",
    "                                                predicates=test_not_in_1,\n",
    "                                                preprocessor=preprocess_triple_text,\n",
    "                                                threshold=.7)\n",
    "# baseline\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence aggregation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse structuring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mff = MostFrequentFirstDiscourseStructuring(template_model=te)\n",
    "# Starts from a node without incoming vertices\n",
    "#    and then do a Breadth first search\n",
    "cds = ChainDiscourseStructuring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicalization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LexicalizeAsAligned(da)\n",
    "lp = LexicalizePreprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject(d):\n",
    "    \n",
    "    d['m_subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "# starting from second sentence, apply replace_subject function over data :)\n",
    "\n",
    "#pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, le, processor=replace_subject, nth=0)\n",
    "pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, lp, processor=replace_subject, nth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'Airport', 'eid': 'Id4', 'idx': '0_3', 'ntriples': 1}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Afonso_Pena_International_Airport | ICAO_Location_Identifier | \"SBCT\"\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = test.sample(idx='0_3')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ICAO Location Identifier of Afonso Pena International Airport is SBCT.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.generate([e.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_subject': 'Afonso_Pena_International_Airport',\n",
       " 'm_predicate': 'ICAO_Location_Identifier',\n",
       " 'm_object': '\"SBCT\"'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lexicalize(e.get_data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aaron Bertram associated Band/associated Musical Artist Suburban Legends Alcatraz , is in the genre of Ska punk. , stylistic Origin Ska'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = test.sample()\n",
    "pipe.generate([sample.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'Artist', 'eid': 'Id1488', 'idx': '0_1487', 'ntriples': 3}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Ska_punk | stylisticOrigin | Ska\n",
       "Aaron_Bertram | associatedBand/associatedMusicalArtist | Suburban_Legends\n",
       "Aaron_Bertram | genre | Ska_punk\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 s, sys: 344 ms, total: 6.34 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import codecs\n",
    "\n",
    "with codecs.open(output_filepath, 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English language is spoken in novel Castle.\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Macmillan Publishers is the parent company of Farrar, Straus and Giroux.\r\n",
      "One of John Cowper Powys notable works is Oliver A Glastonbury Romance.\r\n",
      "Soho Press is in the United States.\r\n",
      "The Secret Scripture was published by Faber and Faber.\r\n",
      "Asian Americans are an ethnic group in the United States.\r\n",
      "The English language is spoken in United States.\r\n",
      "Weymouth Sands was preceded by A Glastonbury Romance.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 \"$output_filepath\" | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_9c17c33ef5d01be49d733d80c2fca2e8708bc9e2\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_name\" --team_filepath \"$output_filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 39.89, 75.3/49.0/32.3/21.2 (BP=1.000, ratio=1.068, hyp_len=47712, ref_len=44677)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
