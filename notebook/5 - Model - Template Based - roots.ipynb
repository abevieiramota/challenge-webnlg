{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gets notebook name and commit hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = nb_name.rsplit('.')[0]\n",
    "\n",
    "commit = !git rev-parse HEAD\n",
    "commit = commit[0]\n",
    "\n",
    "model_name = \"{}_{}\".format(nb_name, commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"{}.txt\".format(model_name)\n",
    "\n",
    "import os\n",
    "\n",
    "output_filepath = os.path.join('../data/models', output_filename)\n",
    "\n",
    "model_temp_dir = os.path.join('../tmp/', model_name)\n",
    "\n",
    "bleu_all_cat = os.path.join(model_temp_dir, \"{}_all-cat.txt\".format(model_name))\n",
    "\n",
    "if not os.path.isdir(model_temp_dir):\n",
    "    os.mkdir(model_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate the template based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../script/webnlg.py\n",
    "%run ../script/data_alignment.py\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateExtractionModel:\n",
    "    \n",
    "    def __init__(self, data_alignment_model):\n",
    "    \n",
    "        self.data_alignment_model = data_alignment_model\n",
    "        \n",
    "\n",
    "    def extract_template(self, text, data):\n",
    "\n",
    "        # spacy model\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # TODO: monitor success of subj/obje alignment\n",
    "        m_subject_span, m_object_span = self.data_alignment_model.align_data(doc, data)\n",
    "\n",
    "        # breaks text into char array\n",
    "        text_char = list(text)\n",
    "\n",
    "        # replaces subject text with m_subject placeholder\n",
    "        text_char[m_subject_span.start_char: m_subject_span.end_char] = '{m_subject}'\n",
    "\n",
    "        if m_object_span:\n",
    "            # tests if the object occurs after the subject >\n",
    "            #    if it is the case, you have to adjust the indexes accordingly\n",
    "            if m_subject_span.start_char > m_object_span.end_char:\n",
    "\n",
    "                base = 0\n",
    "            else:\n",
    "                # adjustes the indexes\n",
    "                # length of the extracted subject text\n",
    "                len_subject_text = m_subject_span.end_char - m_subject_span.start_char\n",
    "                # length of the placeholder minus len_subject_text\n",
    "                base = len('{m_subject}') - len_subject_text\n",
    "\n",
    "            # replaces object text with m_object placeholder\n",
    "            text_char[base + m_object_span.start_char: base + m_object_span.end_char] = '{m_object}'\n",
    "\n",
    "        # build template using the char array\n",
    "        return ''.join(text_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = WebNLGCorpus.load('train')\n",
    "\n",
    "# BIAS: use only 1 tripleset size dataset\n",
    "train_1 = train.subset(ntriples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">The Faroese language is spoken in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Denmark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">m_subject: Denmark</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    The Faroese language\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">m_object: : Faroese language</span>\n",
       "</mark>\n",
       " is spoken in Denmark.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample an entry\n",
    "e = train_1.sample(idx='0_210')\n",
    "\n",
    "# data alignment model with token_sort_ratio similarity metric\n",
    "# PARAM/BIAS: similarity_metric \n",
    "# TODO: train changing similarity_metric\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "# template extraction model\n",
    "te = TemplateExtractionModel(da)\n",
    "    \n",
    "# uses the first reference text\n",
    "# it can have more than 1 reference text\n",
    "first_lexicalization = e.ldf.ltext.values.tolist()[0]\n",
    "# uses the first triple\n",
    "first_triple = e.preprocessed_so()[0]\n",
    "\n",
    "da.render_aligned(nlp(first_lexicalization), first_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{m_object} is spoken in {m_subject}.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracts the template\n",
    "template = te.extract_template(first_lexicalization, first_triple)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Portuguese is spoken in Brazil.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.format(**{'m_subject': 'Brazil', 'm_object': 'Portuguese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{m_subject} picks up the {m_object} in the church\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Abelardo Vieira Mota picks up the car in the church'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove lexicalized information not present in data\n",
    "text = 'Eleanor Rigby picks up the rice in the church'\n",
    "data = {'m_subject': 'Eleanor Rigby', \n",
    "        'm_object': 'rice'}\n",
    "\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "te = TemplateExtractionModel(da)\n",
    "\n",
    "template = te.extract_template(text, data)\n",
    "\n",
    "print(template)\n",
    "\n",
    "template.format(**{'m_subject': 'Abelardo Vieira Mota', 'm_object': 'car'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 10s, sys: 3.34 s, total: 3min 14s\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio)\n",
    "te = TemplateExtractionModel(da)\n",
    "\n",
    "template_db = defaultdict(list)\n",
    "\n",
    "#! BIAS: using only train_1 sentences\n",
    "# for each sentence, extracts template\n",
    "for entry in train_1:\n",
    "    \n",
    "    for text in entry.ldf.ltext.tolist():\n",
    "        # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "        #    will be only one element)\n",
    "        data = entry.preprocessed_so()[0]\n",
    "        predicate = data['m_predicate']\n",
    "\n",
    "        template = te.extract_template(text, data)\n",
    "\n",
    "        # add to db\n",
    "        template_db[predicate].append((entry.edf.idx.values[0], template))\n",
    "    \n",
    "# most frequent template\n",
    "# BIAS: not necessarily are the better ones -> they can be a flawed one, like the ones without m_object\n",
    "for k, templates in template_db.items():\n",
    "    \n",
    "    template_db[k] = Counter([v[1] for v in templates]).most_common(1)[0][0]\n",
    "    \n",
    "predicates_in_db = list(template_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{m_object} is spoken in {m_subject}.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_db['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do I have one predicate for each predicate in test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WebNLGCorpus.load('test_no_lex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 197 predicates in test which don't have a template\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: how to deal with?\n",
    "predicates_in_test = set(test.mdf.m_predicate.tolist())\n",
    "predicates_w_template = template_db.keys()\n",
    "\n",
    "\"There are {} predicates in test which don't have a template\".format(len(predicates_in_test ^ predicates_w_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, let them fall back to the nearest predicate and then to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.Logger('TemplateBasedModel')\n",
    "\n",
    "\n",
    "# gets the nearest predicate in template_db\n",
    "# BIAS: similarity metric for predicate fallback\n",
    "# TODO: parameterize\n",
    "def get_nearest_predicate(predicate):\n",
    "    \n",
    "    distances = [(in_db, similarity.jaro_winkler(predicate, in_db)) for in_db in predicates_in_db]\n",
    "    \n",
    "    return min(distances, key=lambda v: v[1])\n",
    "\n",
    "# generates the sentences\n",
    "def generate_sentences(entry):\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    for triple in entry.preprocessed_so():\n",
    "\n",
    "        m_predicate = triple['m_predicate']\n",
    "\n",
    "        if m_predicate in template_db:\n",
    "            \n",
    "            template = template_db[m_predicate]\n",
    "\n",
    "            text = template.format(**triple)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            nearest_predicate, similarity = get_nearest_predicate(m_predicate)\n",
    "            \n",
    "            if similarity > .4:\n",
    "                \n",
    "                logger.warning(\"Fallback nearest predicate for predicate %s\", m_predicate)\n",
    "                \n",
    "                template = template_db[nearest_predicate]\n",
    "\n",
    "                text = template.format(**triple)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                logger.warning(\"Fallback baseline for predicate %s\", m_predicate)\n",
    "                \n",
    "                text = '{m_subject} {m_predicate} {m_object}'.format(**triple)\n",
    "\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>mtext</th>\n",
       "      <th>m_subject</th>\n",
       "      <th>m_predicate</th>\n",
       "      <th>m_object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | ISSN_number | \"0567-7920\"</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>ISSN_number</td>\n",
       "      <td>\"0567-7920\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | LCCN_number | 60040714</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>LCCN_number</td>\n",
       "      <td>60040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0_419</td>\n",
       "      <td>Acta_Palaeontologica_Polonica | abbreviation | \"Acta Palaeontol. Pol.\"</td>\n",
       "      <td>Acta_Palaeontologica_Polonica</td>\n",
       "      <td>abbreviation</td>\n",
       "      <td>\"Acta Palaeontol. Pol.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  \\\n",
       "639  0_419   \n",
       "640  0_419   \n",
       "641  0_419   \n",
       "\n",
       "                                                                      mtext  \\\n",
       "639               Acta_Palaeontologica_Polonica | ISSN_number | \"0567-7920\"   \n",
       "640                  Acta_Palaeontologica_Polonica | LCCN_number | 60040714   \n",
       "641  Acta_Palaeontologica_Polonica | abbreviation | \"Acta Palaeontol. Pol.\"   \n",
       "\n",
       "                         m_subject   m_predicate                 m_object  \n",
       "639  Acta_Palaeontologica_Polonica   ISSN_number              \"0567-7920\"  \n",
       "640  Acta_Palaeontologica_Polonica   LCCN_number                 60040714  \n",
       "641  Acta_Palaeontologica_Polonica  abbreviation  \"Acta Palaeontol. Pol.\"  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = test.sample(idx='0_419')\n",
    "\n",
    "test_sample.mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The ISSN number of Acta Palaeontologica Polonica is 0567-7920.',\n",
       " 'Acta Palaeontologica Polonica has the LCCN number 60040714.',\n",
       " 'Acta Palaeontologica Polonica is abbreviated to Acta Palaeontol. Pol..']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "generate_sentences(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating texts for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "with codecs.open(output_filepath, 'w', 'utf-8') as f:\n",
    "    \n",
    "    for entry in test:\n",
    "        \n",
    "        # generates one sentence per triple\n",
    "        entry_sentences = generate_sentences(entry)\n",
    "        \n",
    "        # BIAS: aggregation\n",
    "        # TODO: how to deal with?\n",
    "        entry_text = ' '.join(entry_sentences)\n",
    "        \n",
    "        f.write(entry_text)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language is spoken in Castle (novel).\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Macmillan Publishers is the parent company of Farrar, Straus and Giroux.\r\n",
      "One of John Cowper Powys notable works is A Glastonbury Romance.\r\n",
      "Soho Press is located in United States.\r\n",
      "The Secret Scripture is published by Faber and Faber.\r\n",
      "Asian Americans are an ethnic group in United States.\r\n",
      "English language is spoken in United States.\r\n",
      "Weymouth Sands is preceded by A Glastonbury Romance.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 \"$output_filepath\" | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_6dd871aa81bea6fcaf35b71faf932af2e82e17e6\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_name\" --team_filepath \"$output_filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 38.08, 66.4/45.8/31.7/21.8 (BP=1.000, ratio=1.124, hyp_len=50693, ref_len=45098)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
