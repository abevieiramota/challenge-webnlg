{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets notebook name and commit hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = nb_name.rsplit('.')[0]\n",
    "\n",
    "commit = !git rev-parse HEAD\n",
    "commit = commit[0]\n",
    "\n",
    "model_name = \"{}_{}\".format(nb_name, commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"{}.txt\".format(model_name)\n",
    "model_filename = \"{}\".format(model_name)\n",
    "log_filename = \"{}.log\".format(model_name)\n",
    "\n",
    "import os\n",
    "\n",
    "output_filepath = os.path.join('../data/models', output_filename)\n",
    "model_filepath = os.path.join('../data/models', model_filename)\n",
    "log_filepath = os.path.join('../data/models', log_filename)\n",
    "\n",
    "model_temp_dir = os.path.join('../tmp/', model_name)\n",
    "\n",
    "bleu_all_cat = os.path.join(model_temp_dir, \"{}_all-cat.txt\".format(model_name))\n",
    "\n",
    "if not os.path.isdir('../tmp'):\n",
    "    os.mkdir('../tmp')\n",
    "    \n",
    "if not os.path.isdir(model_temp_dir):\n",
    "    os.mkdir(model_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 - Model - Template Based - roots_4e9245cbbc05e4fdf0de404fbada20d8294127b2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, \n",
    "                    level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from webnlg import WebNLGCorpus\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "train = WebNLGCorpus.load('train')\n",
    "dev = WebNLGCorpus.load('dev')\n",
    "test = WebNLGCorpus.load('test_no_lex')\n",
    "\n",
    "# BIAS: use only 1 tripleset size dataset\n",
    "train_1 = train.subset(ntriples=1)\n",
    "dev_1 = dev.subset(ntriples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in train+dev and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 m_predicates in train+dev not present in train_1+dev_1.\n",
      "They are:\n",
      "\n",
      "has to its northwest\n",
      "served\n",
      "gemstone\n",
      "5th_runway_SurfaceType\n",
      "architecture\n",
      "servingSize\n",
      "neighboringMunicipality\n",
      "numberOfRooms\n",
      "has to its southeast\n"
     ]
    }
   ],
   "source": [
    "train_dev_1_predicates = set(train_1.mdf.m_predicate.unique()).union(dev_1.mdf.m_predicate.unique())\n",
    "train_dev_predicates = set(train.mdf.m_predicate.unique()).union(dev.mdf.m_predicate.unique())\n",
    "\n",
    "all_not_in_1 = train_dev_predicates.difference(train_dev_1_predicates)\n",
    "\n",
    "print(\"There are {} m_predicates in train+dev not present in train_1+dev_1.\\nThey are:\\n\\n{}\".format(\n",
    "      len(all_not_in_1), '\\n'.join(all_not_in_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in test and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 117 predicates in test, from 300, which don't have a template\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_in_test = set(test.mdf.m_predicate.unique())\n",
    "test_not_in_1 = predicates_in_test.difference(train_dev_1_predicates)\n",
    "\n",
    "\"There are {} predicates in test, from {}, which don't have a template\".format(len(test_not_in_1),\n",
    "                                                                               len(predicates_in_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the predicate doesn't exist, fall back to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 844 ms, sys: 375 ms, total: 1.22 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentence_generation import FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator\n",
    "from data_alignment import RootDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from webnlg import preprocess_triple_text\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textacy import similarity\n",
    "\n",
    "\n",
    "da = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 1min 16s, total: 3min 16s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "te = TemplateExtractor(da)\n",
    "\n",
    "#! BIAS: using only train_1 sentences\n",
    "# for each sentence, extracts template\n",
    "texts = chain.from_iterable((entry.ldf.ltext.tolist() for entry in chain(train_1, dev_1)))\n",
    "# to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "#    will be only one element)\n",
    "datas = chain.from_iterable(([entry.get_data(preprocessor=preprocess_triple_text)[0]] * entry.ldf.shape[0] for entry in chain(train_1, dev_1)))\n",
    "\n",
    "te.fit(texts, datas)\n",
    "\n",
    "TemplateExtractor.save(te, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tte = TemplateExtractor.load(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "\n",
    "npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                similarity_metric=similarity.levenshtein,\n",
    "                                                predicates=test.mdf.m_predicate.unique().tolist(),\n",
    "                                                preprocessor=preprocess_triple_text,\n",
    "                                                threshold=.5)\n",
    "\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['m_subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, processor=replace_subject, nth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'Astronaut', 'eid': 'Id40', 'idx': '32_39', 'ntriples': 4}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Buzz_Aldrin | birthPlace | Glen_Ridge,_New_Jersey\n",
       "Buzz_Aldrin | was a crew member of | Apollo_11\n",
       "Buzz_Aldrin | almaMater | \"Massachusetts Institute of Technology, Sc.D. 1963\"\n",
       "Buzz_Aldrin | birthDate | \"1930-01-20\"\n",
       "\n",
       "\n",
       "\tLexicalizations:\n",
       "\n",
       "Buzz Aldrin was born in Glen Ridge, New Jersey on 1930-01-20. He attended the Massachusetts Institute of Technology obtaining a Sc.D in 1963. He was a crew member on Apollo 11.\n",
       "Buzz Aldrin was born in Glen Ridge, New Jersey on 20 January 1930. He graduated from MIT ScD in 1963 and was a crew member of Apollo 11."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = train.sample(idx='32_39')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buzz Aldrin was born in Glen Ridge, New Jersey. , served as a crew member of Apollo 11. Massachusetts Institute of Technology, Sc.D. 1963 is , almaMater. , was born on 1930-01-20.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.generate([e.get_data()])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 s, sys: 344 ms, total: 6.12 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import codecs\n",
    "\n",
    "with codecs.open(output_filepath, 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language is spoken in Castle (novel).\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Macmillan Publishers is the parent company of Farrar, Straus and Giroux.\r\n",
      "A Glastonbury Romance is a notable work by John Cowper Powys.\r\n",
      "Soho Press is located in United States.\r\n",
      "The Secret Scripture is published by Faber and Faber.\r\n",
      "Asian Americans are an ethnic group in United States.\r\n",
      "English language is spoken in United States.\r\n",
      "Weymouth Sands is preceded by A Glastonbury Romance.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 \"$output_filepath\" | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_4e9245cbbc05e4fdf0de404fbada20d8294127b2\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_name\" --team_filepath \"$output_filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 40.12, 75.6/49.0/32.5/21.5 (BP=1.000, ratio=1.037, hyp_len=45726, ref_len=44096)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
