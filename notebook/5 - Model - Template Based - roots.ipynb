{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets notebook name and commit hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = nb_name.rsplit('.')[0]\n",
    "\n",
    "commit = !git rev-parse HEAD\n",
    "commit = commit[0]\n",
    "\n",
    "model_name = \"{}_{}\".format(nb_name, commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"{}.txt\".format(model_name)\n",
    "model_filename = \"{}\".format(model_name)\n",
    "log_filename = \"{}.log\".format(model_name)\n",
    "\n",
    "import os\n",
    "\n",
    "output_filepath = os.path.join('../data/models', output_filename)\n",
    "model_filepath = os.path.join('../data/models', model_filename)\n",
    "log_filepath = os.path.join('../data/models', log_filename)\n",
    "\n",
    "model_temp_dir = os.path.join('../tmp/', model_name)\n",
    "\n",
    "bleu_all_cat = os.path.join(model_temp_dir, \"{}_all-cat.txt\".format(model_name))\n",
    "\n",
    "if not os.path.isdir('../tmp'):\n",
    "    os.mkdir('../tmp')\n",
    "    \n",
    "if not os.path.isdir(model_temp_dir):\n",
    "    os.mkdir(model_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=log_filepath, \n",
    "                    level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from webnlg import WebNLGCorpus\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "train_dev = WebNLGCorpus.load(['train', 'dev'])\n",
    "test = WebNLGCorpus.load('test_no_lex')\n",
    "\n",
    "# BIAS: use only 1 tripleset size dataset\n",
    "train_dev_1 = train_dev.subset(ntriples=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in train+dev and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 m_predicates in train+dev not present in train_1+dev_1.\n",
      "They are:\n",
      "\n",
      "gemstone\n",
      "has to its northwest\n",
      "5th_runway_SurfaceType\n",
      "numberOfRooms\n",
      "neighboringMunicipality\n",
      "served\n",
      "servingSize\n",
      "has to its southeast\n",
      "architecture\n"
     ]
    }
   ],
   "source": [
    "train_dev_1_predicates = set(train_dev_1.mdf.m_predicate.unique())\n",
    "train_dev_predicates = set(train_dev.mdf.m_predicate.unique())\n",
    "\n",
    "all_not_in_1 = train_dev_predicates.difference(train_dev_1_predicates)\n",
    "\n",
    "print(\"There are {} m_predicates in train+dev not present in train_1+dev_1.\\nThey are:\\n\\n{}\".format(\n",
    "      len(all_not_in_1), '\\n'.join(all_not_in_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many m_predicates exists in test and not in train_1+dev_1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 117 predicates in test, from 300, which don't have a template\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_in_test = set(test.mdf.m_predicate.unique())\n",
    "test_not_in_1 = predicates_in_test.difference(train_dev_1_predicates)\n",
    "\n",
    "\"There are {} predicates in test, from {}, which don't have a template\".format(len(test_not_in_1),\n",
    "                                                                               len(predicates_in_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the predicate doesn't exist, fall back to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_generation import FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned,LexicalizePreprocessed\n",
    "from webnlg import preprocess_triple_text\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textacy import similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Alignment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 16s, sys: 8min 14s, total: 27min 31s\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "rda = RootDataAlignmentModel(similarity.levenshtein, nlp)\n",
    "ngramda = NGramDataAlignmentModel(3, similarity.levenshtein, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da1 = FallBackDataAlignmentModel(models=[ngramda, spoda, rda])\n",
    "da2 = FallBackDataAlignmentModel(models=[spoda, ngramda, rda])\n",
    "da3 = FallBackDataAlignmentModel(models=[rda, ngramda, spoda])\n",
    "\n",
    "\n",
    "das = [da1, da2, da3]\n",
    "\n",
    "tes = []\n",
    "\n",
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    filepath = f'{model_filepath}{i}'\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        te = TemplateExtractor.load(filepath)\n",
    "    else:\n",
    "        te = TemplateExtractor(da)\n",
    "\n",
    "        # texts from entries\n",
    "        texts = train_dev_1.ldf.ltext.tolist()\n",
    "\n",
    "        # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "        #    will be only one element)\n",
    "\n",
    "        # for each entry you have one or more verbalizations\n",
    "        #    you have to repeat the tripleset N times, N = number of verbalizations\n",
    "        datas = chain.from_iterable([\\\n",
    "            # data for entry\n",
    "            [entry.get_data()[0]] \\\n",
    "                 * \\\n",
    "            # number of verbalizations\n",
    "            entry.ldf.shape[0] for entry in train_dev_1])\n",
    "\n",
    "        te.fit(texts, datas)\n",
    "\n",
    "        TemplateExtractor.save(te, filepath)\n",
    "        \n",
    "    tes.append(te)\n",
    "    \n",
    "    \n",
    "    # uses the most frequente template\n",
    "    mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "    # uses the nearest predicate templates\n",
    "    #    precalculate nearests for test_not_in_1\n",
    "    npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                    similarity_metric=similarity.levenshtein,\n",
    "                                                    predicates=test_not_in_1,\n",
    "                                                    preprocessor=preprocess_triple_text,\n",
    "                                                    threshold=threshold)\n",
    "    # baseline\n",
    "    jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "    sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])\n",
    "\n",
    "    text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "    #mff = MostFrequentFirstDiscourseStructuring(template_model=te)\n",
    "    # Starts from a node without incoming vertices\n",
    "    #    and then do a Breadth first search\n",
    "    cds = ChainDiscourseStructuring()\n",
    "\n",
    "    le = LexicalizeAsAligned(da)\n",
    "    lp = LexicalizePreprocessed()\n",
    "\n",
    "    def replace_subject(d):\n",
    "\n",
    "        d['m_subject'] = ','\n",
    "\n",
    "        return d\n",
    "\n",
    "    # starting from second sentence, apply replace_subject function over data :)\n",
    "\n",
    "    #pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, le, processor=replace_subject, nth=0)\n",
    "    pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, lp, processor=replace_subject, nth=0)\n",
    "\n",
    "    import codecs\n",
    "\n",
    "    with codecs.open(filepath, 'w', 'utf-8') as f:\n",
    "\n",
    "        for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "\n",
    "            f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c0\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c1\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c2\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c3\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c4\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c5\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c6\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c7\n",
      "Files creating finished for:  5 - Model - Template Based - roots_310dd152c335dfb79d9d63de09095d7f70ce7c7c8\n"
     ]
    }
   ],
   "source": [
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    filepath = f'{model_filepath}{i}'\n",
    "    model_namee = f'{model_name}{i}'\n",
    "\n",
    "    !python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_namee\" --team_filepath \"$filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 36.97, 70.8/45.1/29.9/19.6 (BP=1.000, ratio=1.138, hyp_len=52032, ref_len=45719)\n",
      "BLEU = 38.57, 73.4/47.3/31.2/20.5 (BP=1.000, ratio=1.097, hyp_len=49259, ref_len=44910)\n",
      "BLEU = 38.94, 74.1/47.8/31.4/20.7 (BP=1.000, ratio=1.085, hyp_len=48519, ref_len=44714)\n",
      "BLEU = 32.02, 65.7/39.7/25.3/16.0 (BP=1.000, ratio=1.175, hyp_len=53718, ref_len=45703)\n",
      "BLEU = 33.33, 68.4/41.6/26.2/16.6 (BP=1.000, ratio=1.129, hyp_len=50663, ref_len=44889)\n",
      "BLEU = 33.80, 69.2/42.2/26.5/16.9 (BP=1.000, ratio=1.113, hyp_len=49718, ref_len=44672)\n",
      "BLEU = 39.40, 74.6/48.1/32.1/20.9 (BP=1.000, ratio=1.061, hyp_len=47444, ref_len=44702)\n",
      "BLEU = 40.52, 76.6/49.8/32.9/21.5 (BP=1.000, ratio=1.030, hyp_len=45505, ref_len=44163)\n",
      "BLEU = 40.84, 77.2/50.2/33.1/21.7 (BP=1.000, ratio=1.021, hyp_len=44946, ref_len=44016)\n"
     ]
    }
   ],
   "source": [
    "for i, (da, threshold) in enumerate(product(das, [.3, .5, .8])):\n",
    "    \n",
    "    bleu_all_cat_filepath = os.path.join(model_temp_dir, f\"{model_name}{i}_all-cat.txt\")\n",
    "\n",
    "    !../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat_filepath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rda = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "ngramda = NGramDataAlignmentModel(4, similarity.levenshtein, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da = FallBackDataAlignmentModel(models=[ngramda, spoda, rda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 1min 14s, total: 3min 21s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if os.path.exists(model_filepath):\n",
    "    te = TemplateExtractor.load(model_filepath)\n",
    "else:\n",
    "    te = TemplateExtractor(da)\n",
    "\n",
    "    # texts from entries\n",
    "    texts = train_dev_1.ldf.ltext.tolist()\n",
    "\n",
    "    # to dictionary of s, o; [0] because to_dict returns a list of dicts(and, in this case, there\n",
    "    #    will be only one element)\n",
    "\n",
    "    # for each entry you have one or more verbalizations\n",
    "    #    you have to repeat the tripleset N times, N = number of verbalizations\n",
    "    datas = chain.from_iterable([\\\n",
    "        # data for entry\n",
    "        [entry.get_data()[0]] \\\n",
    "             * \\\n",
    "        # number of verbalizations\n",
    "        entry.ldf.shape[0] for entry in train_dev_1])\n",
    "\n",
    "    te.fit(texts, datas)\n",
    "\n",
    "    TemplateExtractor.save(te, model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the most frequente template\n",
    "mft = MostFrequentTemplateSentenceGenerator(te, preprocessor=preprocess_triple_text)\n",
    "# uses the nearest predicate templates\n",
    "#    precalculate nearests for test_not_in_1\n",
    "npt = NearestPredicateTemplateSentenceGenerator(template_sentence_generator=mft,\n",
    "                                                similarity_metric=similarity.levenshtein,\n",
    "                                                predicates=test_not_in_1,\n",
    "                                                preprocessor=preprocess_triple_text,\n",
    "                                                threshold=.7)\n",
    "# baseline\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, npt, jjt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence aggregation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse structuring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mff = MostFrequentFirstDiscourseStructuring(template_model=te)\n",
    "# Starts from a node without incoming vertices\n",
    "#    and then do a Breadth first search\n",
    "cds = ChainDiscourseStructuring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicalization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LexicalizeAsAligned(da)\n",
    "lp = LexicalizePreprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject(d):\n",
    "    \n",
    "    d['m_subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "# starting from second sentence, apply replace_subject function over data :)\n",
    "\n",
    "#pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, le, processor=replace_subject, nth=0)\n",
    "pipe = IfAfterNthProcessPipelineTextGenerator(sent_pipe, text_agg, cds, lp, processor=replace_subject, nth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'Airport', 'eid': 'Id4', 'idx': '0_3', 'ntriples': 1}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Afonso_Pena_International_Airport | ICAO_Location_Identifier | \"SBCT\"\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = test.sample(idx='0_3')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ICAO Location Identifier of Afonso Pena International Airport is SBCT.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.generate([e.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_subject': 'Afonso Pena International Airport',\n",
       " 'm_predicate': 'ICAO_Location_Identifier',\n",
       " 'm_object': '\"SBCT\"'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lexicalize(e.get_data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total area of Atlantic City, New Jersey is square kilometres 44.125.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = test.sample()\n",
    "pipe.generate([sample.get_data()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Triple info: {'category': 'City', 'eid': 'Id172', 'idx': '0_171', 'ntriples': 1}\n",
       "\n",
       "\tModified triples:\n",
       "\n",
       "Atlantic_City,_New_Jersey | areaTotal | 44.125 (square kilometres)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.3 s, sys: 266 ms, total: 6.56 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import codecs\n",
    "\n",
    "with codecs.open(output_filepath, 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in pipe.generate((entry.get_data() for entry in test)):\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English language is spoken in novel Castle.\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Macmillan Publishers is the parent company of Farrar, Straus and Giroux.\r\n",
      "One of John Cowper Powys notable works is Oliver A Glastonbury Romance.\r\n",
      "Soho Press is in the United States.\r\n",
      "The Secret Scripture was published by Faber and Faber.\r\n",
      "Asian Americans are an ethnic group in the United States.\r\n",
      "The English language is spoken in United States.\r\n",
      "Weymouth Sands was preceded by A Glastonbury Romance.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 \"$output_filepath\" | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  5 - Model - Template Based - roots_c952c4774651e01f72450b3ac8ebcde98ab9f157\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name \"$model_name\" --team_filepath \"$output_filepath\" --outdir \"$model_temp_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 39.89, 75.3/49.0/32.3/21.2 (BP=1.000, ratio=1.068, hyp_len=47712, ref_len=44677)\r\n"
     ]
    }
   ],
   "source": [
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
