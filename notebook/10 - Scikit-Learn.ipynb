{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from itertools import repeat, chain\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "from webnlg import WebNLGCorpus\n",
    "from lexicalization import preprocess_so\n",
    "\n",
    "from content_selection import SelectAllContentSelection, SelectFirstNContentSelection\n",
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator, FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring, DoesntSortDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned, LexicalizePreprocessed, FallBackLexicalize\n",
    "from webnlg import preprocess_triple_text\n",
    "from textacy import similarity\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGBaseline(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, sep=None, preprocess_data=None):\n",
    "        \n",
    "        self.sep = sep\n",
    "        self.preprocess_data = preprocess_data\n",
    "\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_entry(self, x):\n",
    "        \n",
    "        sens = []\n",
    "        \n",
    "        for data in x:\n",
    "                \n",
    "            m_s = self.preprocess_data(data['subject'])\n",
    "            m_p = self.preprocess_data(data['predicate'])\n",
    "            m_o = self.preprocess_data(data['object'])\n",
    "        \n",
    "            sens.append(f'{m_s} {m_p} {m_o}')\n",
    "        \n",
    "        return self.sep.join(sens)\n",
    "    \n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        return [self.predict_entry(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WebNLGCorpus.load(\"test_with_lex\")\n",
    "train = WebNLGCorpus.load(['train', 'dev'])\n",
    "\n",
    "X = np.array([t.get_data() for t in train])\n",
    "y = np.array([t.lexes() for t in train])\n",
    "\n",
    "X_test = np.array([t.get_data() for t in test])\n",
    "y_test = np.array([t.lexes() for t in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=NLGBaseline(preprocess_data=None, sep=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'sep': [' '], 'preprocess_data': [<function preprocess_so at 0x7f4be2e73048>]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=make_scorer(bleu_),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "param_grid = {'sep': [' '],\n",
    "              'preprocess_data': [preprocess_so]}\n",
    "\n",
    "cv = GridSearchCV(NLGBaseline(), param_grid, scoring=bleu, cv=4)\n",
    "\n",
    "cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.17828, std: 0.02259, params: {'preprocess_data': <function preprocess_so at 0x7f4be2e73048>, 'sep': ' '}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGTemplateBased(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, content_selection=None, data_alignment=None, discourse_structurer=None, processor=lambda x: x, sentence_generator=None, lexicalizer=None, sentence_aggregator=None, nth=-1):\n",
    "        \n",
    "        self.content_selection = content_selection\n",
    "        self.data_alignment = data_alignment\n",
    "        self.discourse_structurer = discourse_structurer\n",
    "        self.sentence_generator = sentence_generator\n",
    "        self.lexicalizer = lexicalizer\n",
    "        self.sentence_aggregator = sentence_aggregator\n",
    "        self.nth = nth\n",
    "        self.processor = processor\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        self.template_extractor = TemplateExtractor(self.data_alignment)\n",
    "        \n",
    "        # 1-triple size\n",
    "        X_1, y_1 = zip(*[(x[0], y_) for (x, y_) in zip(X, y) if len(x) == 1])\n",
    "        \n",
    "        X_1 = list(chain.from_iterable([x_1]*len(y_1_) for (x_1, y_1_) in zip(X_1, y_1)))\n",
    "        y_1 = list(chain.from_iterable(y_1))\n",
    "        \n",
    "        self.template_extractor.fit(y_1, X_1)\n",
    "        self.sentence_generator.fit(self.template_extractor)\n",
    "        self.lexicalizer.fit(self.data_alignment)\n",
    "        self.discourse_structurer.fit(self.template_extractor)\n",
    "    \n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        generated_texts = []\n",
    "\n",
    "        for entry in X:\n",
    "            \n",
    "            selected_data = self.content_selection.select(entry)\n",
    "\n",
    "            sorted_data = self.discourse_structurer.sort(selected_data)\n",
    "            \n",
    "            sentences = []\n",
    "            for i, d in enumerate(sorted_data):\n",
    "\n",
    "                if i > self.nth:\n",
    "\n",
    "                    d = self.processor(d)\n",
    "                    \n",
    "                lexicalized = self.lexicalizer.lexicalize(d)\n",
    "                \n",
    "                sentence = self.sentence_generator.generate(lexicalized)\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                \n",
    "            text = self.sentence_aggregator.aggregate(sentences)\n",
    "\n",
    "            generated_texts.append(text)\n",
    "\n",
    "        return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import discourse_structuring\n",
    "reload(discourse_structuring)\n",
    "from discourse_structuring import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Balder (comics Character)  alternativeName  Balder Odinson , creator Stan Lee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content Selection\n",
    "csall = SelectAllContentSelection()\n",
    "\n",
    "# Data Alignment\n",
    "rda = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "ngramda = NGramDataAlignmentModel(3, similarity.jaro_winkler, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "da = FallBackDataAlignmentModel(models=[spoda, ngramda, rda])\n",
    "\n",
    "# Sentence Generation\n",
    "mft = MostFrequentTemplateSentenceGenerator()\n",
    "# npt = NearestPredicateTemplateSentenceGenerator(mft, similarity.token_sort_ratio, .6)\n",
    "jjt = JustJoinTripleSentenceGenerator()\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, jjt])\n",
    "\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "#cds = ChainDiscourseStructuring()\n",
    "cds = MostFrequentFirstDiscourseStructuring()\n",
    "\n",
    "le = LexicalizeAsAligned()\n",
    "lp = LexicalizePreprocessed(preprocess_triple_text)\n",
    "fle = FallBackLexicalize([le, lp])\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "nlg = NLGTemplateBased(content_selection=csall,\n",
    "                       data_alignment=da,\n",
    "                       discourse_structurer=cds,\n",
    "                       processor=replace_subject,\n",
    "                       sentence_generator=sent_pipe,\n",
    "                       sentence_aggregator=text_agg,\n",
    "                       lexicalizer=fle,\n",
    "                       nth=0)\n",
    "\n",
    "nlg.fit(X[:200], y[:200])\n",
    "\n",
    "nlg.predict([[{'idx': '11_19',\n",
    "  'mtext': 'Balder_(comicsCharacter) | alternativeName | \"Balder Odinson\"',\n",
    "  'subject': 'Balder_(comicsCharacter) ',\n",
    "  'predicate': ' alternativeName ',\n",
    "  'object': ' \"Balder Odinson\"'},\n",
    " {'idx': '11_19',\n",
    "  'mtext': 'Balder_(comicsCharacter) | creator | Stan_Lee',\n",
    "  'subject': 'Balder_(comicsCharacter) ',\n",
    "  'predicate': 'creator',\n",
    "  'object': ' Stan_Lee'}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 52min 8s, sys: 516 ms, total: 1h 52min 9s\n",
      "Wall time: 30min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "css = [SelectAllContentSelection(), SelectFirstNContentSelection(n=1)]\n",
    "\n",
    "rdas = [RootDataAlignmentModel(sim, nlp) for sim in [similarity.token_sort_ratio, similarity.levenshtein]]\n",
    "ngramdas = [NGramDataAlignmentModel(3, sim, nlp) for sim in [similarity.token_sort_ratio, similarity.levenshtein]]\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "das = [FallBackDataAlignmentModel(models) for models in product(ngramdas, rdas, [spoda])]\n",
    "\n",
    "# Sentence Generation\n",
    "mft = MostFrequentTemplateSentenceGenerator()\n",
    "# npt = NearestPredicateTemplateSentenceGenerator(mft, similarity.token_sort_ratio, .6)\n",
    "jjt = JustJoinTripleSentenceGenerator()\n",
    "sent_pipes = [FallBackPipelineSentenceGenerator([mft, jjt]), FallBackPipelineSentenceGenerator([jjt, mft])]\n",
    "\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "# poor results ChainDiscourseStructuring()\n",
    "cdss = [MostFrequentFirstDiscourseStructuring(), ChainDiscourseStructuring(), DoesntSortDiscourseStructuring()]\n",
    "\n",
    "la = LexicalizeAsAligned()\n",
    "lp = LexicalizePreprocessed(preprocess_triple_text)\n",
    "les = [FallBackLexicalize([la, lp])]\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "# scorer\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "\n",
    "# grid\n",
    "param_grid = {'content_selection': css,\n",
    "              'data_alignment': das,\n",
    "              'discourse_structurer': cdss,\n",
    "              'sentence_generator': sent_pipes,\n",
    "              'sentence_aggregator': [text_agg],\n",
    "              'lexicalizer': les,\n",
    "              'nth': [0],\n",
    "              'processor': [replace_subject]}\n",
    "\n",
    "# data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X, y, random_state=200)\n",
    "\n",
    "cv = GridSearchCV(NLGTemplateBased(), param_grid, scoring=bleu, cv=2, return_train_score=True)\n",
    "\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_content_selection', 'param_data_alignment', 'param_discourse_structurer', 'param_lexicalizer', 'param_nth', 'param_processor', 'param_sentence_aggregator', 'param_sentence_generator', 'params', 'split0_test_score', 'split1_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU mean:\n",
      "[0.09630577 0.09630577]\n",
      "\n",
      "Content Selection:\n",
      "[SelectFirstNContentSelection(n=1,\n",
      "               sort_function=<function SelectFirstNContentSelection.<lambda> at 0x7f520fa51378>)\n",
      " SelectFirstNContentSelection(n=1,\n",
      "               sort_function=<function SelectFirstNContentSelection.<lambda> at 0x7f520fa51378>)]\n",
      "\n",
      "Data Alignment:\n",
      "[FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>)))\n",
      " FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function levenshtein at 0x7f528a986e18>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>)))]\n",
      "\n",
      "Discourse Structuring:\n",
      "[ChainDiscourseStructuring() ChainDiscourseStructuring()]\n",
      "\n",
      "Lexicalizer:\n",
      "[FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...1f98>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f52c8098a60>)])\n",
      " FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...1f98>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f52c8098a60>)])]\n",
      "\n",
      "Nth:\n",
      "[0 0]\n",
      "\n",
      "Processor:\n",
      "[<function replace_subject at 0x7f520f7856a8>\n",
      " <function replace_subject at 0x7f520f7856a8>]\n",
      "\n",
      "Sentence Aggregator:\n",
      "[JustJoinSentencesSentenceAggregator(sep=' ')\n",
      " JustJoinSentencesSentenceAggregator(sep=' ')]\n",
      "\n",
      "Sentence Generator:\n",
      "[FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])\n",
      " FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])]\n"
     ]
    }
   ],
   "source": [
    "pos = np.where(results['rank_test_score'] == 21)\n",
    "#pos = results['rank_test_score'].argmax()\n",
    "\n",
    "print(f\"Test BLEU mean:\\n{results['mean_test_score'][pos]}\\n\")\n",
    "print(f\"Content Selection:\\n{results['param_content_selection'][pos]}\\n\")\n",
    "print(f\"Data Alignment:\\n{results['param_data_alignment'][pos]}\\n\")\n",
    "print(f\"Discourse Structuring:\\n{results['param_discourse_structurer'][pos]}\\n\")\n",
    "print(f\"Lexicalizer:\\n{results['param_lexicalizer'][pos]}\\n\")\n",
    "print(f\"Nth:\\n{results['param_nth'][pos]}\\n\")\n",
    "print(f\"Processor:\\n{results['param_processor'][pos]}\\n\")\n",
    "print(f\"Sentence Aggregator:\\n{results['param_sentence_aggregator'][pos]}\\n\")\n",
    "print(f\"Sentence Generator:\\n{results['param_sentence_generator'][pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_selection': SelectAllContentSelection(),\n",
       " 'data_alignment': FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
       "             nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
       "             similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
       "             similarity_metric=<function levenshtein at 0x7f528a986e18>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f528a981f98>))),\n",
       " 'discourse_structurer': ChainDiscourseStructuring(),\n",
       " 'lexicalizer': FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
       "             nlp=<spacy.lang.en.English object at 0x7f528a981f98>,\n",
       "             similarity_metric=<function token_sort_ratio at 0x7f528a986f28>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...1f98>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f52c8098a60>)]),\n",
       " 'nth': 0,\n",
       " 'processor': <function __main__.replace_subject(d)>,\n",
       " 'sentence_aggregator': JustJoinSentencesSentenceAggregator(sep=' '),\n",
       " 'sentence_generator': FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22956497219816455"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open('../data/models/scikit-learn', 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language is spoken in Castle (novel).\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Johns Farrar, Straus and Giroux is the parent company of the Johns Macmillan Publishers Press.\r\n",
      "A Glastonbury Romance John John Cowper Powys notable works is Oliver Glendower.\r\n",
      "Soho Press is located in United States.\r\n",
      "The Secret Scripture Quarterly is published by SAGE Publications for the Samuel Faber and Faber School of Management, Cornell University.\r\n",
      "The Asian Americans are an ethnic group in the United States.\r\n",
      "English language is spoken in United States.\r\n",
      "1634: The Weymouth Sands was preceded by 1634: The A Glastonbury Romance.\r\n",
      "A.C. Chievo Verona are managed by Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 ../data/models/scikit-learn | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../tmp/scikit-learn’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  scikit-learn\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name scikit-learn --team_filepath ../data/models/scikit-learn --outdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 34.36, 70.6/45.4/30.1/20.1 (BP=0.921, ratio=0.924, hyp_len=36775, ref_len=39810)\r\n"
     ]
    }
   ],
   "source": [
    "bleu_all_cat = \"../tmp/scikit-learn/scikit-learn_all-cat.txt\"\n",
    "\n",
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
