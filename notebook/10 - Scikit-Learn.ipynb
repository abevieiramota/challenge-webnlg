{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "from webnlg import WebNLGCorpus\n",
    "from lexicalization import preprocess_so\n",
    "\n",
    "# score -> \n",
    "#    -> [OK] NLTK BLEU\n",
    "#    -> Competition BLEU\n",
    "\n",
    "# fit -> \n",
    "#    -> my model\n",
    "\n",
    "# predict -> \n",
    "#    -> my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGBaseline(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, sep=None, preprocess_data=None):\n",
    "        \n",
    "        self.sep = sep\n",
    "        self.preprocess_data = preprocess_data\n",
    "\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_entry(self, x):\n",
    "        \n",
    "        sens = []\n",
    "        \n",
    "        for data in x:\n",
    "                \n",
    "            m_s = self.preprocess_data(data['subject'])\n",
    "            m_p = self.preprocess_data(data['predicate'])\n",
    "            m_o = self.preprocess_data(data['object'])\n",
    "        \n",
    "            sens.append(f'{m_s} {m_p} {m_o}')\n",
    "        \n",
    "        return self.sep.join(sens)\n",
    "    \n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        return [self.predict_entry(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otimiza esses iterators maxo!\n",
    "test = WebNLGCorpus.load(\"test_with_lex\")\n",
    "train = WebNLGCorpus.load(['train', 'dev'])\n",
    "\n",
    "X = [t.get_data() for t in train]\n",
    "y = [t.lexes() for t in train]\n",
    "\n",
    "X_test = [t.get_data() for t in test]\n",
    "y_test = [t.lexes() for t in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=NLGBaseline(preprocess_data=None, sep=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'sep': [' '], 'preprocess_data': [<function <lambda> at 0x7f6b94d14d08>, <function preprocess_so at 0x7f6b95201c80>]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=make_scorer(bleu_),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "param_grid = {'sep': [' '],\n",
    "              'preprocess_data': [lambda x: x, preprocess_so]}\n",
    "\n",
    "cv = GridSearchCV(NLGBaseline(), param_grid, scoring=bleu, cv=4)\n",
    "\n",
    "cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.04550, std: 0.01609, params: {'preprocess_data': <function <lambda> at 0x7f6b94d14d08>, 'sep': ' '},\n",
       " mean: 0.18200, std: 0.02697, params: {'preprocess_data': <function preprocess_so at 0x7f6b95201c80>, 'sep': ' '}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from template_extraction import TemplateExtractor\n",
    "from itertools import chain\n",
    "\n",
    "class NLGTemplateBased(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, data_alignment=None, discourse_structurer=None, processor=lambda x: x, sentence_generator=None, lexicalizer=None, sentence_aggregator=None, nth=-1):\n",
    "        \n",
    "        self.data_alignment = data_alignment\n",
    "        self.discourse_structurer = discourse_structurer\n",
    "        self.sentence_generator = sentence_generator\n",
    "        self.lexicalizer = lexicalizer\n",
    "        self.sentence_aggregator = sentence_aggregator\n",
    "        self.nth = nth\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        self.template_extractor = TemplateExtractor(self.data_alignment)\n",
    "        \n",
    "        # 1-triple size\n",
    "        X_1, y_1 = zip(*[(x[0], y_) for (x, y_) in zip(X, y) if len(x) == 1])\n",
    "        \n",
    "        X_1 = list(chain.from_iterable([x_1]*len(y_1_) for (x_1, y_1_) in zip(X_1, y_1)))\n",
    "        y_1 = list(chain.from_iterable(y_1))\n",
    "        \n",
    "        self.template_extractor.fit(y_1, X_1)\n",
    "        self.sentence_generator.fit(self.template_extractor)\n",
    "        self.lexicalizer.fit(self.data_alignment)\n",
    "        self.discourse_structurer.fit(self.template_extractor)\n",
    "    \n",
    "    \n",
    "    def predict_entry(self, x):\n",
    "        \n",
    "        sens = []\n",
    "        \n",
    "        for data in x:\n",
    "                \n",
    "            m_s = self.preprocess_data(data['subject'])\n",
    "            m_p = self.preprocess_data(data['predicate'])\n",
    "            m_o = self.preprocess_data(data['object'])\n",
    "        \n",
    "            sens.append(f'{m_s} {m_p} {m_o}')\n",
    "        \n",
    "        return self.sep.join(sens)\n",
    "    \n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        generated_texts = []\n",
    "\n",
    "        for entry in X:\n",
    "\n",
    "            sorted_data = self.discourse_structurer.sort(entry)\n",
    "            \n",
    "            sentences = []\n",
    "            for i, d in enumerate(sorted_data):\n",
    "\n",
    "                if i > self.nth:\n",
    "\n",
    "                    d = self.processor(d)\n",
    "                \n",
    "                sentence = self.sentence_generator.generate(self.lexicalizer.lexicalize(d))\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                \n",
    "            text = self.sentence_aggregator.aggregate(sentences)\n",
    "\n",
    "            generated_texts.append(text)\n",
    "\n",
    "        return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator, FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned, LexicalizePreprocessed\n",
    "from webnlg import preprocess_triple_text\n",
    "from textacy import similarity\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amsterdam Airport Schiphol 5th runway Surface Type Asphalt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rda1 = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "rda2 = RootDataAlignmentModel(similarity.levenshtein, nlp)\n",
    "rda3 = RootDataAlignmentModel(similarity.jaro_winkler, nlp)\n",
    "ngramda41 = NGramDataAlignmentModel(4, similarity.levenshtein, nlp)\n",
    "ngramda42 = NGramDataAlignmentModel(4, similarity.token_sort_ratio, nlp)\n",
    "ngramda43 = NGramDataAlignmentModel(4, similarity.jaro_winkler, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "# Subject Predicate Object\n",
    "# > Ngram \n",
    "# > > Dependency tree Root\n",
    "da111 = FallBackDataAlignmentModel(models=[ngramda41, spoda, rda1])\n",
    "da112 = FallBackDataAlignmentModel(models=[ngramda41, spoda, rda2])\n",
    "da113 = FallBackDataAlignmentModel(models=[ngramda41, spoda, rda3])\n",
    "da121 = FallBackDataAlignmentModel(models=[ngramda42, spoda, rda1])\n",
    "da122 = FallBackDataAlignmentModel(models=[ngramda42, spoda, rda2])\n",
    "da123 = FallBackDataAlignmentModel(models=[ngramda42, spoda, rda3])\n",
    "da131 = FallBackDataAlignmentModel(models=[ngramda43, spoda, rda1])\n",
    "da132 = FallBackDataAlignmentModel(models=[ngramda43, spoda, rda2])\n",
    "da133 = FallBackDataAlignmentModel(models=[ngramda43, spoda, rda3])\n",
    "\n",
    "# uses the most frequente template\n",
    "mft = MostFrequentTemplateSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, jjt])\n",
    "\n",
    "# baseline\n",
    "jjt = JustJoinTripleSentenceGenerator(preprocessor=preprocess_triple_text)\n",
    "\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "#cds = ChainDiscourseStructuring()\n",
    "cds = MostFrequentFirstDiscourseStructuring()\n",
    "\n",
    "le = LexicalizeAsAligned()\n",
    "lp = LexicalizePreprocessed()\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "nlg = NLGTemplateBased(data_alignment=da111,\n",
    "                       discourse_structurer=cds,\n",
    "                       processor=replace_subject,\n",
    "                       sentence_generator=sent_pipe,\n",
    "                       sentence_aggregator=text_agg,\n",
    "                       lexicalizer=lp,\n",
    "                       nth=0)\n",
    "\n",
    "nlg.fit(X[:100], y[:100])\n",
    "\n",
    "nlg.predict([[{'subject': 'Amsterdam_Airport_Schiphol',\n",
    "   'predicate': '5th_runway_SurfaceType',\n",
    "   'object': '\"Asphalt\"'}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 57s, sys: 1.73 s, total: 48min 59s\n",
      "Wall time: 13min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "param_grid = {'data_alignment': [da111, da112, da113, da121, da122, da123, da131, da132, da133],\n",
    "              'discourse_structurer': [cds],\n",
    "              'processor': [replace_subject],\n",
    "              'sentence_generator': [sent_pipe],\n",
    "              'sentence_aggregator': [text_agg],\n",
    "              'lexicalizer': [lp, le],\n",
    "              'nth': [1]}\n",
    "\n",
    "cv = GridSearchCV(NLGTemplateBased(), param_grid, scoring=bleu, cv=2)\n",
    "\n",
    "cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLGTemplateBased(data_alignment=FallBackDataAlignmentModel(models=[NGramDataAlignmentModel(max_n=4,\n",
       "            nlp=<spacy.lang.en.English object at 0x7fdfc1246f98>,\n",
       "            similarity_metric=<function levenshtein at 0x7fdfb9d4e598>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7fdfc1246f98>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7fdfc1246f98>,\n",
       "            similarity_metric=<function token_sort_ratio at 0x7fdfb9d4e6a8>)]),\n",
       "         discourse_structurer=MostFrequentFirstDiscourseStructuring(template_model=None),\n",
       "         lexicalizer=LexicalizePreprocessed(), nth=1,\n",
       "         processor=<function replace_subject at 0x7fdfb9d4eea0>,\n",
       "         sentence_aggregator=JustJoinSentencesSentenceAggregator(sep=' '),\n",
       "         sentence_generator=<sentence_generation.FallBackPipelineSentenceGenerator object at 0x7fdf9f9c37b8>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2664650325591825"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open('../data/models/scikit-learn', 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English language is spoken in novel Castle.\r\n",
      "Eric Flint was born in Burbank, California.\r\n",
      "Farrar, Straus and Giroux University is the parent company of the Macmillan Publishers University Press.\r\n",
      "One of John Cowper Powys Powys notable works is A Glastonbury Romance.\r\n",
      "Soho Press is located in United States.\r\n",
      "Faber and Faber is the publisher of The Secret Scripture Science Quarterly.\r\n",
      "Asian Americans are an ethnic group United States U.S.\r\n",
      "English language is spoken in United States.\r\n",
      "DeMarce short stories in A Glastonbury Romance Gazettes preceded 1634: Weymouth Sands Crisis.\r\n",
      "The manager of A.C. Chievo Verona is Rolando Maran.\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 ../data/models/scikit-learn | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  scikit-learn\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name scikit-learn --team_filepath ../data/models/scikit-learn --outdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 35.52, 68.0/43.9/28.6/18.6 (BP=1.000, ratio=1.154, hyp_len=52141, ref_len=45177)\r\n"
     ]
    }
   ],
   "source": [
    "bleu_all_cat = \"../tmp/scikit-learn/scikit-learn_all-cat.txt\"\n",
    "\n",
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
