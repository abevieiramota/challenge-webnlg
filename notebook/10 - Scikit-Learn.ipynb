{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from itertools import repeat, chain\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "os.sys.path.insert(0, '../script')\n",
    "from webnlg import WebNLGCorpus\n",
    "from lexicalization import preprocess_so\n",
    "\n",
    "from content_selection import SelectAllContentSelection, SelectFirstNContentSelection\n",
    "from sentence_aggregation import JustJoinSentencesSentenceAggregator\n",
    "from sentence_generation import JustJoinTripleSentenceGenerator, MostFrequentTemplateSentenceGenerator, FallBackPipelineSentenceGenerator, NearestPredicateTemplateSentenceGenerator\n",
    "from discourse_structuring import MostFrequentFirstDiscourseStructuring, ChainDiscourseStructuring, DoesntSortDiscourseStructuring\n",
    "from data_alignment import RootDataAlignmentModel, NGramDataAlignmentModel, SPODataAlignmentModel, FallBackDataAlignmentModel\n",
    "from template_extraction import TemplateExtractor\n",
    "from text_generation import IfAfterNthProcessPipelineTextGenerator\n",
    "from lexicalization import LexicalizeAsAligned, LexicalizePreprocessed, FallBackLexicalize\n",
    "from webnlg import preprocess_triple_text\n",
    "from textacy import similarity\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGBaseline(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, sep=None, preprocess_data=None):\n",
    "        \n",
    "        self.sep = sep\n",
    "        self.preprocess_data = preprocess_data\n",
    "\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_entry(self, x):\n",
    "        \n",
    "        sens = []\n",
    "        \n",
    "        for data in x:\n",
    "                \n",
    "            m_s = self.preprocess_data(data['subject'])\n",
    "            m_p = self.preprocess_data(data['predicate'])\n",
    "            m_o = self.preprocess_data(data['object'])\n",
    "        \n",
    "            sens.append(f'{m_s} {m_p} {m_o}')\n",
    "        \n",
    "        return self.sep.join(sens)\n",
    "    \n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        return [self.predict_entry(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WebNLGCorpus.load(\"test_with_lex\")\n",
    "train = WebNLGCorpus.load(['train', 'dev'])\n",
    "\n",
    "X = np.array([t.get_data() for t in train])\n",
    "y = np.array([t.lexes() for t in train])\n",
    "\n",
    "X_test = np.array([t.get_data() for t in test])\n",
    "y_test = np.array([t.lexes() for t in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=NLGBaseline(preprocess_data=None, sep=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'sep': [' '], 'preprocess_data': [<function preprocess_so at 0x7f4be2e73048>]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=make_scorer(bleu_),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "param_grid = {'sep': [' '],\n",
    "              'preprocess_data': [preprocess_so]}\n",
    "\n",
    "cv = GridSearchCV(NLGBaseline(), param_grid, scoring=bleu, cv=4)\n",
    "\n",
    "cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.17828, std: 0.02259, params: {'preprocess_data': <function preprocess_so at 0x7f4be2e73048>, 'sep': ' '}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import preprocess\n",
    "\n",
    "class NLGTemplateBased(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, content_selection=None, data_alignment=None, discourse_structurer=None, processor=lambda x: x, sentence_generator=None, lexicalizer=None, sentence_aggregator=None, nth=-1):\n",
    "        \n",
    "        self.content_selection = content_selection\n",
    "        self.data_alignment = data_alignment\n",
    "        self.discourse_structurer = discourse_structurer\n",
    "        self.sentence_generator = sentence_generator\n",
    "        self.lexicalizer = lexicalizer\n",
    "        self.sentence_aggregator = sentence_aggregator\n",
    "        self.nth = nth\n",
    "        self.processor = processor\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        self.template_extractor = TemplateExtractor(self.data_alignment)\n",
    "        \n",
    "        # 1-triple size\n",
    "        X_1, y_1 = zip(*[(x[0], y_) for (x, y_) in zip(X, y) if len(x) == 1])\n",
    "        \n",
    "        X_1 = list(chain.from_iterable([x_1]*len(y_1_) for (x_1, y_1_) in zip(X_1, y_1)))\n",
    "        y_1 = list(chain.from_iterable(y_1))\n",
    "        \n",
    "        self.template_extractor.fit(y_1, X_1)\n",
    "        self.sentence_generator.fit(self.template_extractor)\n",
    "        self.lexicalizer.fit(self.data_alignment)\n",
    "        self.discourse_structurer.fit(self.template_extractor)\n",
    "    \n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        generated_texts = []\n",
    "\n",
    "        for entry in X:\n",
    "            \n",
    "            selected_data = self.content_selection.select(entry)\n",
    "            \n",
    "            preprocessed_data = [{k: preprocess.remove_punct(v, marks='\"\"').strip() for k, v in data.items()} for data in selected_data]\n",
    "\n",
    "            sorted_data = self.discourse_structurer.sort(preprocessed_data)\n",
    "            \n",
    "            sentences = []\n",
    "            for i, d in enumerate(sorted_data):\n",
    "\n",
    "                if i > self.nth:\n",
    "\n",
    "                    d = self.processor(d)\n",
    "                    \n",
    "                lexicalized = self.lexicalizer.lexicalize(d)\n",
    "                \n",
    "                sentence = self.sentence_generator.generate(lexicalized)\n",
    "                \n",
    "                sentences.append(sentence)\n",
    "                \n",
    "            text = self.sentence_aggregator.aggregate(sentences)\n",
    "\n",
    "            generated_texts.append(text)\n",
    "\n",
    "        return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Balder (comics Character) alternativeName Balder Odinson , creator Stan Lee']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content Selection\n",
    "csall = SelectAllContentSelection()\n",
    "\n",
    "# Data Alignment\n",
    "rda = RootDataAlignmentModel(similarity.token_sort_ratio, nlp)\n",
    "ngramda = NGramDataAlignmentModel(3, similarity.jaro_winkler, nlp)\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "\n",
    "da = FallBackDataAlignmentModel(models=[spoda, ngramda, rda])\n",
    "\n",
    "# Sentence Generation\n",
    "mft = MostFrequentTemplateSentenceGenerator()\n",
    "# npt = NearestPredicateTemplateSentenceGenerator(mft, similarity.token_sort_ratio, .6)\n",
    "jjt = JustJoinTripleSentenceGenerator()\n",
    "sent_pipe = FallBackPipelineSentenceGenerator([mft, jjt])\n",
    "\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "#cds = ChainDiscourseStructuring()\n",
    "cds = MostFrequentFirstDiscourseStructuring()\n",
    "\n",
    "le = LexicalizeAsAligned()\n",
    "lp = LexicalizePreprocessed(preprocess_triple_text)\n",
    "fle = FallBackLexicalize([le, lp])\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "nlg = NLGTemplateBased(content_selection=csall,\n",
    "                       data_alignment=da,\n",
    "                       discourse_structurer=cds,\n",
    "                       processor=replace_subject,\n",
    "                       sentence_generator=sent_pipe,\n",
    "                       sentence_aggregator=text_agg,\n",
    "                       lexicalizer=fle,\n",
    "                       nth=0)\n",
    "\n",
    "nlg.fit(X[:200], y[:200])\n",
    "\n",
    "nlg.predict([[{'idx': '11_19',\n",
    "  'mtext': 'Balder_(comicsCharacter) | alternativeName | \"Balder Odinson\"',\n",
    "  'subject': 'Balder_(comicsCharacter) ',\n",
    "  'predicate': ' alternativeName ',\n",
    "  'object': ' \"Balder Odinson\"'},\n",
    " {'idx': '11_19',\n",
    "  'mtext': 'Balder_(comicsCharacter) | creator | Stan_Lee',\n",
    "  'subject': 'Balder_(comicsCharacter) ',\n",
    "  'predicate': 'creator',\n",
    "  'object': ' Stan_Lee'}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abevieiramota/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:542: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'end_char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webnlg/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-38bfe03cc090>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_alignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/1513 MX5_7/Documents/GitHub/challenge-webnlg/script/template_extraction.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, texts, datas)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpredicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# add to db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/1513 MX5_7/Documents/GitHub/challenge-webnlg/script/template_extraction.py\u001b[0m in \u001b[0;36mextract_template\u001b[0;34m(self, text, data)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# tests if the object occurs after the subject >\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#    if it is the case, you have to adjust the indexes accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msubject_span\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_char\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mobject_span\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_char\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'end_char'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "css = [SelectAllContentSelection()]\n",
    "\n",
    "rdas = [RootDataAlignmentModel(sim, nlp) for sim in [similarity.token_sort_ratio]]\n",
    "ngramdas = [NGramDataAlignmentModel(n, similarity.token_sort_ratio, nlp) for n in range(3, 5)]\n",
    "spoda = SPODataAlignmentModel(nlp)\n",
    "das = [FallBackDataAlignmentModel(models) for models in product(ngramdas, rdas, [spoda])]\n",
    "\n",
    "# Sentence Generation\n",
    "mft = MostFrequentTemplateSentenceGenerator()\n",
    "# npt = NearestPredicateTemplateSentenceGenerator(mft, similarity.token_sort_ratio, .6)\n",
    "jjt = JustJoinTripleSentenceGenerator()\n",
    "sent_pipes = [FallBackPipelineSentenceGenerator([mft, jjt]), FallBackPipelineSentenceGenerator([jjt, mft])]\n",
    "\n",
    "text_agg = JustJoinSentencesSentenceAggregator(sep=' ')\n",
    "\n",
    "# poor results ChainDiscourseStructuring()\n",
    "cdss = [MostFrequentFirstDiscourseStructuring(), ChainDiscourseStructuring(), DoesntSortDiscourseStructuring()]\n",
    "\n",
    "la = LexicalizeAsAligned()\n",
    "lp = LexicalizePreprocessed(preprocess_triple_text)\n",
    "les = [FallBackLexicalize([la, lp])]\n",
    "\n",
    "def replace_subject(d):\n",
    "    \n",
    "    d['subject'] = ','\n",
    "    \n",
    "    return d\n",
    "\n",
    "# scorer\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "def bleu_(y_true, y_pred):\n",
    "    \n",
    "    y_true_ = [[wt.tokenize(ref) for ref in refs] for refs in y_true]\n",
    "    y_pred_ = [wt.tokenize(hypothesi) for hypothesi in y_pred]\n",
    "    \n",
    "    return corpus_bleu(y_true_, y_pred_)\n",
    "\n",
    "bleu = make_scorer(bleu_)\n",
    "\n",
    "# grid\n",
    "param_grid = {'content_selection': css,\n",
    "              'data_alignment': das,\n",
    "              'discourse_structurer': cdss,\n",
    "              'sentence_generator': sent_pipes,\n",
    "              'sentence_aggregator': [text_agg],\n",
    "              'lexicalizer': les,\n",
    "              'nth': [0],\n",
    "              'processor': [replace_subject]}\n",
    "\n",
    "# data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X, y, random_state=200)\n",
    "\n",
    "cv = GridSearchCV(NLGTemplateBased(), param_grid, scoring=bleu, cv=2, return_train_score=True)\n",
    "\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_content_selection', 'param_data_alignment', 'param_discourse_structurer', 'param_lexicalizer', 'param_nth', 'param_processor', 'param_sentence_aggregator', 'param_sentence_generator', 'params', 'split0_test_score', 'split1_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU mean:\n",
      "[0.14555327 0.14555327 0.14555327 0.14555327]\n",
      "\n",
      "Content Selection:\n",
      "[SelectAllContentSelection() SelectAllContentSelection()\n",
      " SelectAllContentSelection() SelectAllContentSelection()]\n",
      "\n",
      "Data Alignment:\n",
      "[FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>)))\n",
      " FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>)))\n",
      " FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=4,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>)))\n",
      " FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=4,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>)))]\n",
      "\n",
      "Discourse Structuring:\n",
      "[ChainDiscourseStructuring() ChainDiscourseStructuring()\n",
      " ChainDiscourseStructuring() ChainDiscourseStructuring()]\n",
      "\n",
      "Lexicalizer:\n",
      "[FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...e6d8>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f99144481e0>)])\n",
      " FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...e6d8>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f99144481e0>)])\n",
      " FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...e6d8>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f99144481e0>)])\n",
      " FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
      "            nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
      "            similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...e6d8>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f99144481e0>)])]\n",
      "\n",
      "Nth:\n",
      "[0 0 0 0]\n",
      "\n",
      "Processor:\n",
      "[<function replace_subject at 0x7f98648b0c80>\n",
      " <function replace_subject at 0x7f98648b0c80>\n",
      " <function replace_subject at 0x7f98648b0c80>\n",
      " <function replace_subject at 0x7f98648b0c80>]\n",
      "\n",
      "Sentence Aggregator:\n",
      "[JustJoinSentencesSentenceAggregator(sep=' ')\n",
      " JustJoinSentencesSentenceAggregator(sep=' ')\n",
      " JustJoinSentencesSentenceAggregator(sep=' ')\n",
      " JustJoinSentencesSentenceAggregator(sep=' ')]\n",
      "\n",
      "Sentence Generator:\n",
      "[FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])\n",
      " FallBackPipelineSentenceGenerator(models=[JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}'), MostFrequentTemplateSentenceGenerator()])\n",
      " FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])\n",
      " FallBackPipelineSentenceGenerator(models=[JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}'), MostFrequentTemplateSentenceGenerator()])]\n"
     ]
    }
   ],
   "source": [
    "pos = np.where(results['rank_test_score'] == 1)\n",
    "#pos = results['rank_test_score'].argmax()\n",
    "\n",
    "print(f\"Test BLEU mean:\\n{results['mean_test_score'][pos]}\\n\")\n",
    "print(f\"Content Selection:\\n{results['param_content_selection'][pos]}\\n\")\n",
    "print(f\"Data Alignment:\\n{results['param_data_alignment'][pos]}\\n\")\n",
    "print(f\"Discourse Structuring:\\n{results['param_discourse_structurer'][pos]}\\n\")\n",
    "print(f\"Lexicalizer:\\n{results['param_lexicalizer'][pos]}\\n\")\n",
    "print(f\"Nth:\\n{results['param_nth'][pos]}\\n\")\n",
    "print(f\"Processor:\\n{results['param_processor'][pos]}\\n\")\n",
    "print(f\"Sentence Aggregator:\\n{results['param_sentence_aggregator'][pos]}\\n\")\n",
    "print(f\"Sentence Generator:\\n{results['param_sentence_generator'][pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_selection': SelectAllContentSelection(),\n",
       " 'data_alignment': FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
       "             nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
       "             similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
       "             similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), SPODataAlignmentModel(nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>))),\n",
       " 'discourse_structurer': ChainDiscourseStructuring(),\n",
       " 'lexicalizer': FallBackLexicalize(models=[LexicalizeAsAligned(data_alignment=FallBackDataAlignmentModel(models=(NGramDataAlignmentModel(max_n=3,\n",
       "             nlp=<spacy.lang.en.English object at 0x7f98dc7ae6d8>,\n",
       "             similarity_metric=<function token_sort_ratio at 0x7f98dc7bf510>), RootDataAlignmentModel(nlp=<spacy.lang.en.Eng...e6d8>)))), LexicalizePreprocessed(preprocessor=<function preprocess_triple_text at 0x7f99144481e0>)]),\n",
       " 'nth': 0,\n",
       " 'processor': <function __main__.replace_subject(d)>,\n",
       " 'sentence_aggregator': JustJoinSentencesSentenceAggregator(sep=' '),\n",
       " 'sentence_generator': FallBackPipelineSentenceGenerator(models=[MostFrequentTemplateSentenceGenerator(), JustJoinTripleSentenceGenerator(sentence_template='{subject} {predicate} {object}')])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14681765955986625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open('../data/models/scikit-learn', 'w', 'utf-8') as f:\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        f.write(\"{}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Castle (novel) language English language\r\n",
      "Eric Flint birthPlace Burbank, California\r\n",
      "Farrar, Straus and Giroux parentCompany Macmillan Publishers\r\n",
      "John Cowper Powys notableWork A Glastonbury Romance\r\n",
      "Soho Press country United States\r\n",
      "The Secret Scripture publisher Faber and Faber\r\n",
      "United States ethnicGroup Asian Americans\r\n",
      "United States language English language\r\n",
      "Weymouth Sands precededBy A Glastonbury Romance\r\n",
      "A.C. Chievo Verona manager Rolando Maran\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 ../data/models/scikit-learn | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../tmp/scikit-learn’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files creating finished for:  scikit-learn\r\n"
     ]
    }
   ],
   "source": [
    "!python ../evaluation/webnlg2017/webnlg-automatic-evaluation-v2/evaluation_v2.py --team_name scikit-learn --team_filepath ../data/models/scikit-learn --outdir ../tmp/scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 25.45, 74.0/44.0/25.4/15.0 (BP=0.763, ratio=0.787, hyp_len=30926, ref_len=39274)\r\n"
     ]
    }
   ],
   "source": [
    "bleu_all_cat = \"../tmp/scikit-learn/scikit-learn_all-cat.txt\"\n",
    "\n",
    "!../evaluation/webnlg2017/webnlg-baseline-master/multi-bleu.perl -lc ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference0.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference1.lex ../evaluation/webnlg2017/webnlg-automatic-evaluation/references/gold-all-cat-reference2.lex < \"$bleu_all_cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
