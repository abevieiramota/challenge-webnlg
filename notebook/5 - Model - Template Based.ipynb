{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../script/webnlg.py\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = WebNLGCorpus.load(dataset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Template Model\n",
    "\n",
    "ntriples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.subset(ntriples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The wizard, oz, people]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('The wizard of oz is killing people')\n",
    "\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The wizard'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:2].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the distance a plugable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(node):\n",
    "    \n",
    "    return doc[node.left_edge.i: node.right_edge.i + 1]\n",
    "\n",
    "\n",
    "roots = [token for token in doc if token.head == token]\n",
    "\n",
    "nodes = []\n",
    "\n",
    "data = ['wizard of oz', 'people']\n",
    "\n",
    "distances = []\n",
    "\n",
    "for root in roots:\n",
    "    \n",
    "    nodes.append(root)\n",
    "    \n",
    "    distances_root = []\n",
    "    \n",
    "    for d in data:\n",
    "        \n",
    "        distances_root.append(edit_distance(d, get_span(root).text))\n",
    "        \n",
    "    distances.append(distances_root)\n",
    "    \n",
    "    roots.extend(root.lefts)\n",
    "    roots.extend(root.rights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wizard of oz</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>killing</th>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizard</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         wizard of oz  people\n",
       "killing            22      28\n",
       "wizard              4      14\n",
       "is                 11       6\n",
       "people             11       0\n",
       "The                12       5\n",
       "of                  7       6\n",
       "oz                 10       5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(distances, index=nodes, columns=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by_groupname(m):\n",
    "    \n",
    "    return \"{{{}}}\".format(next((k for k, v in m.groupdict().items() if v)))\n",
    "\n",
    "DISTANCE_THRESHOLD = 200\n",
    "\n",
    "#TODO: parameterize distance metric\n",
    "nnc_logger = logging.getLogger(\"nearest_noun_chunks\")\n",
    "def nearest_noun_chunks(doc, m_subject, m_object):\n",
    "    \n",
    "    map_text_into_function = {m_subject: 'm_subject',\n",
    "                              m_object: 'm_object'}\n",
    "    \n",
    "    distances_s = [(m_subject, nc, edit_distance(m_subject, nc.text)) for nc in doc.noun_chunks]\n",
    "    distances_o = [(m_object, nc, edit_distance(m_object, nc.text)) for nc in doc.noun_chunks]\n",
    "    \n",
    "    distances = distances_s + distances_o\n",
    "    \n",
    "    if not distances:\n",
    "        \n",
    "        raise Exception(\"doc without sufficient noun chunks: {}\".format(doc))\n",
    "    \n",
    "    nnc_logger.debug(distances)\n",
    "    \n",
    "    min_distance_1 = min(distances, key=lambda v: v[2])\n",
    "    \n",
    "    if min_distance_1[2] > DISTANCE_THRESHOLD:\n",
    "        nnc_logger.warning(\"distance threshold: {}\".format(min_distance_1))\n",
    "    \n",
    "    # remove distances from already matched resource\n",
    "    distances_without_m = [v for v in distances if v[1] != min_distance_1[1] and v[0] != min_distance_1[0]]\n",
    "    \n",
    "    if not distances_without_m:\n",
    "        \n",
    "        raise Exception(\"doc without sufficient noun chunks: {}\".format(doc))\n",
    "    \n",
    "    nnc_logger.debug(distances_without_m)\n",
    "    \n",
    "    min_distance_2 = min(distances_without_m, key=lambda v: v[2])\n",
    "    \n",
    "    if min_distance_2[2] > DISTANCE_THRESHOLD:\n",
    "        nnc_logger.warning(\"distance threshold: {}\".format(min_distance_2))\n",
    "\n",
    "    return {map_text_into_function[min_distance_1[0]]: min_distance_1[1],\n",
    "            map_text_into_function[min_distance_2[0]]: min_distance_2[1]}\n",
    "\n",
    "\n",
    "class TemplateExtractor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.logger = logging.getLogger('TemplateExtractor')\n",
    "    \n",
    "    def extract_template(self, text, triple):\n",
    "        \n",
    "        slots = {}\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        ncc = nearest_noun_chunks(doc, triple['m_subject'], triple['m_object'])\n",
    "        ncc_regex_escaped = {k: re.escape(v.text) for k, v in ncc.items()}\n",
    "        \n",
    "        self.logger.debug(ncc)\n",
    "        \n",
    "        # is it necessary to compile?\n",
    "        c = re.compile(\"((?P<m_subject>{m_subject})|(?P<m_object>{m_object}))\".format(**ncc_regex_escaped))\n",
    "\n",
    "        return c.sub(replace_by_groupname, doc.text)\n",
    "    \n",
    "\n",
    "#TODO: search python template libraries\n",
    "class Template(object):\n",
    "    \n",
    "    def __init__(self, template_string):\n",
    "        \n",
    "        self.template_string = template_string\n",
    "        \n",
    "    def fill(self, triple):\n",
    "        \n",
    "        return self.template_string.format(**triple)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        return self.template_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nearest_noun_chunks:distance threshold: ('Derbyshire_Dales', the Derbyshire Dales region, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bakewell tart comes from the Derbyshire Dales region.\n",
      "\n",
      "{'m_object': 'Derbyshire_Dales', 'm_predicate': 'region', 'm_subject': 'Bakewell_tart'}\n",
      "\n",
      "{m_subject} comes from {m_object}.\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "\n",
    "e = train_1.sample()\n",
    "\n",
    "lexe = e.lexes()[0]\n",
    "triple = e.triples()[0]\n",
    "\n",
    "te = TemplateExtractor()\n",
    "t = te.extract_template(lexe, triple)\n",
    "\n",
    "print(lexe)\n",
    "print()\n",
    "print(triple)\n",
    "print()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "* some objects or subjects aren't fully captured\n",
    "    * idx = 5_37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a template database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 2.2 s, total: 2min 20s\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "logger.setLevel(logging.ERROR)\n",
    "template_db = defaultdict(set)\n",
    "\n",
    "lexes_triples = pd.merge(train_1.ldf, train_1.mdf)\n",
    "\n",
    "te = TemplateExtractor()\n",
    "\n",
    "for ix, row in lexes_triples.iterrows():\n",
    "    lexe = row['ltext']\n",
    "    triple = {'m_subject': row['m_subject'],\n",
    "              'm_object': row['m_object'],\n",
    "              'm_predicate': row['m_predicate']\n",
    "             }\n",
    "    try:\n",
    "        t = te.extract_template(lexe, triple)\n",
    "    \n",
    "        template_db[row['m_predicate']].add(t)\n",
    "    except Exception as ex:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many predicates did we get templates for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many predicates are in the train_1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227,)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1.mdf.m_predicate.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many templates per predicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    225.000000\n",
       "mean      13.013333\n",
       "std       21.639390\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        5.000000\n",
       "75%       14.000000\n",
       "max      170.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index = [(m_predicate, len(templates)) for m_predicate, templates in template_db.items()]\n",
    "data = [d[1] for d in data_index]\n",
    "index = [d[0] for d in data_index]\n",
    "\n",
    "stats_on_templates = pd.Series(data=data, index=index)\n",
    "stats_on_templates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's the predicate with most templates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country         170\n",
       "isPartOf        164\n",
       "runwayLength     96\n",
       "location         93\n",
       "leaderName       83\n",
       "language         75\n",
       "club             72\n",
       "runwayName       66\n",
       "leaderTitle      62\n",
       "creator          54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_on_templates.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def split(spo):\n",
    "    \n",
    "    return spo.split(' ')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(spo):\n",
    "    \n",
    "    return ' '.join((lemmatizer.lemmatize(spo_) for spo_ in split(spo)))\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer_ = 'lancaster'\n",
    "\n",
    "if stemmer_ == 'lancaster':\n",
    "    stemmer = LancasterStemmer()\n",
    "elif stemmer == 'porter':\n",
    "    stemmer = PorterStemmer()\n",
    "else:\n",
    "    stemmer = SnowballStemmer()\n",
    "\n",
    "def stem(spo):\n",
    "    \n",
    "    return ' '.join((stemmer.stem(spo_) for spo_ in split(spo)))\n",
    "\n",
    "c_detect_camelcase = re.compile(r'(?<=[a-z])([A-Z])')\n",
    "def split_by_camelcase(spo):\n",
    "    \n",
    "    return c_detect_camelcase.sub(r' \\1', spo)\n",
    "\n",
    "c_chars_to_remove = re.compile(r'[_]')\n",
    "def remove_unwanted_char(spo):\n",
    "    \n",
    "    return c_chars_to_remove.sub(' ', spo)\n",
    "\n",
    "def to_lower(spo):\n",
    "    \n",
    "    return spo.lower()\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "spo_pipeline = [split_by_camelcase, remove_unwanted_char, to_lower, stem]\n",
    "def preprocess_spo(spo):\n",
    "    \n",
    "    return reduce(lambda v, f: f(v), spo_pipeline, spo)\n",
    "    \n",
    "def preprocess_triple(triple):\n",
    "\n",
    "    return {k:preprocess_spo(v) for k, v in triple.items()}    \n",
    "\n",
    "lexe_pipeline = [to_lower, stem]\n",
    "def preprocess_lexe(lexe):\n",
    "    \n",
    "    return reduce(lambda v, f: f(v), lexe_pipeline, lexe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sop(m):\n",
    "    \n",
    "    return \"<{}>\".format(next((k for k, v in m.groupdict().items() if v)))\n",
    "\n",
    "def simple_align(triple, lexe):\n",
    "    \n",
    "    preprocessed_triple = preprocess_triple(triple)\n",
    "    \n",
    "    preprocessed_lexe = preprocess_lexe(lexe)\n",
    "    \n",
    "    regex = '((?P<subject>{m_subject})|(?P<predicate>{m_predicate})|(?P<object>{m_object}))'.format(**preprocessed_triple)\n",
    "    \n",
    "    return re.compile(regex).sub(replace_sop, preprocessed_lexe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the <object> ar the <predicate> in <subject>.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = simple_align(e.triples(kind='dict')[0], e.lexes()[0])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_search_spo = re.compile(r'((?P<object><object>)|(?P<predicate><predicate>)|(?P<subject><subject>))')\n",
    "\n",
    "def count_spo(s):\n",
    "    \n",
    "    return [t[0] for t in c_search_spo.findall(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the character, <subject>es, was <predicate> by <object>.',\n",
       " ['<subject>', '<predicate>', '<object>'],\n",
       " {'m_object': 'Len_Wein', 'm_predicate': 'creator', 'm_subject': 'Aurakles'})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = train.sample()\n",
    "c = simple_align(e.triples(kind='dict')[0], e.lexes()[0])\n",
    "c, count_spo(c), e.triples()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<subject> is a <object>.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': None, 'predicate': None, 'subject': '<subject>'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_search_spo.match(c).groupdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<subject>', '', '', '<subject>'),\n",
       " ('<object>', '<object>', '', ''),\n",
       " ('<predicate>', '', '<predicate>', ''),\n",
       " ('<subject>', '', '', '<subject>')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_search_spo.findall('<subject> <object> <predicate> <subject>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "%run ../script/spacy_util.py\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'m_object': 'Left_Ecology_Freedom',\n",
       "  'm_predicate': 'leaderParty',\n",
       "  'm_subject': 'Gubbio'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'m_object': 'Spain', 'm_predicate': 'country', 'm_subject': 'Arròs_negre'},\n",
      " {'m_object': 'White_rice',\n",
      "  'm_predicate': 'ingredient',\n",
      "  'm_subject': 'Arròs_negre'}]\n",
      "White rice is an ingredient of Arros negre which is a traditional dish from Spain.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    White rice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    an ingredient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " of Arros negre which is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    a traditional dish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Spain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    White rice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    an ingredient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " of Arros negre which is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    a traditional dish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Spain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\"></span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e = train.sample()\n",
    "pprint(e.triples())\n",
    "\n",
    "print(e.lexes()[0])\n",
    "doc = nlp(e.lexes()[0])\n",
    "\n",
    "displacy_noun_chunks(doc)\n",
    "displacy_noun_chunks(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Head</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "      <th>Entity</th>\n",
       "      <th>EntIOB</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>determiner</td>\n",
       "      <td>det</td>\n",
       "      <td>DT</td>\n",
       "      <td>Xxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>Freedom</td>\n",
       "      <td>The</td>\n",
       "      <td>The</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>Freedom</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ecology</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>Freedom</td>\n",
       "      <td>Ecology</td>\n",
       "      <td>Ecology</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>ecology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Freedom</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>are</td>\n",
       "      <td>The</td>\n",
       "      <td>Freedom</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are</td>\n",
       "      <td>verb</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>VBP</td>\n",
       "      <td>xxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>are</td>\n",
       "      <td>The</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>determiner</td>\n",
       "      <td>det</td>\n",
       "      <td>DT</td>\n",
       "      <td>xxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>party</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>leading</td>\n",
       "      <td>verb</td>\n",
       "      <td>amod</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>party</td>\n",
       "      <td>leading</td>\n",
       "      <td>leading</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>party</td>\n",
       "      <td>noun</td>\n",
       "      <td>attr</td>\n",
       "      <td>NN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>are</td>\n",
       "      <td>the</td>\n",
       "      <td>Gubbio</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>adposition</td>\n",
       "      <td>prep</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>party</td>\n",
       "      <td>in</td>\n",
       "      <td>Gubbio</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gubbio</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>pobj</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>X</td>\n",
       "      <td>--</td>\n",
       "      <td>in</td>\n",
       "      <td>Gubbio</td>\n",
       "      <td>Gubbio</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "      <td>gubbio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>are</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text          POS       Dep  Tag  Shape Alpha Stop     Head     Left  \\\n",
       "0       The   determiner       det   DT    Xxx     X   --  Freedom      The   \n",
       "1      Left  proper noun  compound  NNP   Xxxx     X   --  Freedom     Left   \n",
       "2   Ecology  proper noun  compound  NNP  Xxxxx     X   --  Freedom  Ecology   \n",
       "3   Freedom  proper noun     nsubj  NNP  Xxxxx     X   --      are      The   \n",
       "4       are         verb      ROOT  VBP    xxx     X   --      are      The   \n",
       "5       the   determiner       det   DT    xxx     X   --    party      the   \n",
       "6   leading         verb      amod  VBG   xxxx     X   --    party  leading   \n",
       "7     party         noun      attr   NN   xxxx     X   --      are      the   \n",
       "8        in   adposition      prep   IN     xx     X   --    party       in   \n",
       "9    Gubbio  proper noun      pobj  NNP  Xxxxx     X   --       in   Gubbio   \n",
       "10        .  punctuation     punct    .      .    --   --      are        .   \n",
       "\n",
       "      Right Entity EntIOB    Lemma  \n",
       "0       The             O      the  \n",
       "1      Left             O     left  \n",
       "2   Ecology             O  ecology  \n",
       "3   Freedom             O  freedom  \n",
       "4         .             O       be  \n",
       "5       the             O      the  \n",
       "6   leading             O     lead  \n",
       "7    Gubbio             O    party  \n",
       "8    Gubbio             O       in  \n",
       "9    Gubbio    GPE      B   gubbio  \n",
       "10        .             O        .  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_noun_chunks(doc):\n",
    "    \n",
    "    lexe_s = list(doc.text)\n",
    "    for nc in doc.noun_chunks:\n",
    "\n",
    "        lexe_s[nc.start_char: nc.end_char] = '#' * len(nc.__str__())\n",
    "        \n",
    "    return ''.join(lexe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "for _ in range(10):\n",
    "    \n",
    "    e = train.sample()\n",
    "    \n",
    "    for lexe in e.lexes():\n",
    "        \n",
    "        doc = nlp(lexe)\n",
    "        \n",
    "        values.append({\n",
    "            'lexe': lexe,\n",
    "            'noun_chunks': list(doc.noun_chunks),\n",
    "            'masked': mask_noun_chunks(doc),\n",
    "            'triples': e.triples()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values)\n",
    "df.to_html('oi.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'m_object': 'Switzerland',\n",
       "  'm_predicate': 'country',\n",
       "  'm_subject': 'Accademia_di_Architettura_di_Mendrisio'},\n",
       " {'m_object': 'Meride',\n",
       "  'm_predicate': 'neighboringMunicipality',\n",
       "  'm_subject': 'Mendrisio'},\n",
       " {'m_object': 'Mendrisio',\n",
       "  'm_predicate': 'city',\n",
       "  'm_subject': 'Accademia_di_Architettura_di_Mendrisio'},\n",
       " {'m_object': 'Johann_Schneider-Ammann',\n",
       "  'm_predicate': 'leaderName',\n",
       "  'm_subject': 'Switzerland'}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample().triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
